# svm_model_trainer.py

from __future__ import annotations
import pickle
import os
from datetime import datetime
from collections import Counter
import csv
import numpy as np

from sklearn.svm import LinearSVC
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    classification_report,
    make_scorer,
    fbeta_score
)
from sklearn.model_selection import GridSearchCV, cross_val_predict

DEFAULT_CV_FOLDS = 5
PARAM_GRID = {'C': [0.01, 0.1, 1, 10, 100, 1000]}  # mrm svm hyperparameter grid

TRAINING_ROOT_DIR       = "/tmp/svm/training/"
TRAINING_DATA_PATH      = TRAINING_ROOT_DIR + "features_v6.txt"
MODEL_OUTPUT_PATH       = TRAINING_ROOT_DIR + "svm.model"
MODEL_EVAL_REPORT_PATH  = TRAINING_ROOT_DIR + "svm_eval_report.txt"
FEATURE_DEBUG_CSV       = TRAINING_ROOT_DIR + "features_debug.csv"
GRID_SEARCH_RESULTS_CSV = TRAINING_ROOT_DIR + "svm_grid_search_results.csv"

DEBUG_FEATURE_CSV = True
TRAINING_DATA_HEADER = "utterance | features | intent"
ONLY_TRAIN_WITH_ENOUGH_SAMPLES = False

def log_step(message: str):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")

def load_training_data(path: str, header: str) -> tuple[list[dict], list[str], list[str]]:
    with open(path, encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    assert lines[0].replace(" ", "") == header.replace(" ", "")
    feats, labels, utts = [], [], []
    for idx, line in enumerate(lines[1:], start=2):
        parts = [x.strip() for x in line.split('|')]
        if len(parts) != 3:
            print(f"Malformed row on line {idx}: {line}")
            continue
        utt, feature_str, intent = parts
        d = {}
        for pair in feature_str.split('%'):
            if '=' in pair:
                name, val = pair.split('=', 1)
                d[f"{name.strip()}={val.strip()}"] = 1
        feats.append(d)
        labels.append(intent)
        utts.append(utt)
    return feats, labels, utts

def write_feature_debug_csv(features, labels, utterances, vectorizer, path, top_n=15):
    if not DEBUG_FEATURE_CSV:
        return
    log_step("Writing feature debug CSV...")
    X = vectorizer.transform(features)
    fnames = vectorizer.get_feature_names_out()
    totals = np.sum(X, axis=0).A1
    top_idx = np.argsort(totals)[-top_n:][::-1]
    selected = [fnames[i] for i in top_idx]
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', newline='', encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(["utterance", "intent"] + selected)
        for utt, lab, row in zip(utterances, labels, X):
            vec = row.toarray().flatten()
            vals = [int(vec[list(fnames).index(f)]) for f in selected]
            w.writerow([utt, lab] + vals)
    print(f"Feature debug file written: {path}")

def summarize_intents(max_count: int, labels: list[str]):
    cnt = Counter(lab for lab in labels if lab.strip())
    print(f"Intent distribution (â‰¤ {max_count} utterances):")
    for intent, c in sorted(cnt.items()):
        if c <= max_count:
            print(f"{intent}: {c}")

def prepare_data(features: list[dict], labels: list[str]):
    if ONLY_TRAIN_WITH_ENOUGH_SAMPLES:
        log_step("Filtering intents with too few samples")
        cnt = Counter(labels)
        keep = [i for i, lab in enumerate(labels) if cnt[lab] >= DEFAULT_CV_FOLDS]
        features = [features[i] for i in keep]
        labels   = [labels[i]   for i in keep]
        log_step(f"Retained {len(features)} samples")
    feature_vectorizer = DictVectorizer(sparse=True)
    X = feature_vectorizer.fit_transform(features)
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(labels)
    return feature_vectorizer, label_encoder, X, y

def tune_hyperparameters(X, y):
    log_step("Starting GridSearchCV over C with F0.5 scoring (5 folds)...")
    f05 = make_scorer(fbeta_score, beta=0.5)
    grid = GridSearchCV(
        LinearSVC(dual=False, max_iter=10000),
        PARAM_GRID,
        cv=DEFAULT_CV_FOLDS,
        scoring=f05,
    )
    grid.fit(X, y)
    log_step(f"Best C = {grid.best_params_['C']}, Best F0.5 = {grid.best_score_:.4f}")
    return grid

def write_grid_search_results(grid, path: str):
    log_step("Writing grid-search CV results to CSV...")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', newline='', encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(["C", "mean_test_f0.5", "std_test_f0.5"])
        for mean, std, params in zip(
            grid.cv_results_['mean_test_score'],
            grid.cv_results_['std_test_score'],
            grid.cv_results_['params']
        ):
            w.writerow([params['C'], f"{mean:.4f}", f"{std:.4f}"])
    print(f"Grid search results written to {path}")

def save_models_for_each_C(X, y, vectorizer, encoder):
    for C in PARAM_GRID['C']:
        log_step(f"Training final model with C={C}")
        model = LinearSVC(dual=False, C=C, max_iter=10000)
        model.fit(X, y)
        out_path = TRAINING_ROOT_DIR + f"c_models/svm_C_{C}.model"
        write_svm_model_to_file(model, vectorizer, encoder, out_path)

def report_cross_validation_metrics(model, X, y, le):
    log_step(f"Reporting CV metrics with {DEFAULT_CV_FOLDS} folds...")
    y_pred = cross_val_predict(model, X, y, cv=DEFAULT_CV_FOLDS, n_jobs=-1)
    acc = accuracy_score(y, y_pred)
    prec, rec, f1, _ = precision_recall_fscore_support(y, y_pred, average='weighted')
    rpt = classification_report(y, y_pred, target_names=le.classes_)
    print(f"CV Accuracy: {acc:.4f}")
    print(f"CV Precision: {prec:.4f}")
    print(f"CV Recall: {rec:.4f}")
    print(f"CV F1 Score: {f1:.4f}")
    os.makedirs(os.path.dirname(MODEL_EVAL_REPORT_PATH), exist_ok=True)
    with open(MODEL_EVAL_REPORT_PATH, 'w', encoding='utf-8') as f:
        f.write(f"Cross-Validation Report (Folds={DEFAULT_CV_FOLDS})\n")
        f.write(f"Accuracy: {acc:.4f}\nPrecision: {prec:.4f}\nRecall: {rec:.4f}\nF1: {f1:.4f}\n\n")
        f.write(rpt)
    print(f"Evaluation report saved to {MODEL_EVAL_REPORT_PATH}")

def write_svm_model_to_file(model, vectorizer, encoder, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'wb') as f:
        pickle.dump({'model': model, 'vectorizer': vectorizer, 'label_encoder': encoder}, f)
    print(f"Model saved to {path}")

def main():
    # always use 5-fold CV
    cv_folds = DEFAULT_CV_FOLDS

    print("Loading training data...")
    features, labels, utterances = load_training_data(TRAINING_DATA_PATH, TRAINING_DATA_HEADER)
    summarize_intents(5, labels)
    print(f"Loaded {len(features)} samples")

    feature_vectorizer, label_encoder, X, y = prepare_data(features, labels)

    # Hyperparameter tuning (5-fold)
    grid = tune_hyperparameters(X, y)
    write_grid_search_results(grid, GRID_SEARCH_RESULTS_CSV)

    save_models_for_each_C(X, y, feature_vectorizer, label_encoder)

    # Evaluate and save best model (5-fold CV)
    best_model = grid.best_estimator_
    report_cross_validation_metrics(best_model, X, y)
    write_svm_model_to_file(best_model, feature_vectorizer, label_encoder, MODEL_OUTPUT_PATH)

    write_feature_debug_csv(features, labels, utterances, feature_vectorizer, FEATURE_DEBUG_CSV)

if __name__ == "__main__":
    main()