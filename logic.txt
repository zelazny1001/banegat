# svm_trainer.py

from __future__ import annotations
import pickle
import argparse
import os
from datetime import datetime
from collections import Counter
from tqdm import tqdm
import csv
import numpy as np
from sklearn import svm
from sklearn.base import clone
from sklearn.svm import LinearSVC
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report

TRAINING_ROOT_DIR = "/tmp/svm/training/"
TRAINING_DATA_PATH = TRAINING_ROOT_DIR + "features.txt"
CLEANSED_DATA_PATH = TRAINING_ROOT_DIR + "features_cleansed.txt"
MODEL_OUTPUT_PATH = TRAINING_ROOT_DIR + "svm.model"
MODEL_EVAL_REPORT_PATH = TRAINING_ROOT_DIR + "svm_eval_report.txt"
FEATURE_DEBUG_CSV = TRAINING_ROOT_DIR + "features_debug.csv"
DEBUG_FEATURE_CSV = True
TRAINING_DATA_HEADER = "utterance | features | intent"
DEFAULT_CV_FOLDS = 5
ONLY_TRAIN_WITH_INTENTS_THAT_HAVE_AT_LEAST_CV_FOLDS = True

def log_step(message: str):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")

def load_training_data(path: str, header: str) -> tuple[list[dict], list[str], list[str]]:
    with open(path, encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    assert lines[0] == header
    features, labels, utterances = [], [], []
    for idx, line in enumerate(lines[1:], start=2):
        try:
            parts = [x.strip() for x in line.split('|')]
            if len(parts) != 3:
                print(f"Malformed row on line {idx}: {line}")
                continue
            feats = {pair.strip(): 1 for pair in parts[1].split('%') if '=' in pair}
            features.append(feats)
            labels.append(parts[2])
            utterances.append(parts[0])
        except Exception as e:
            print(f"Error processing line {idx}: {line} – {e}")
    return features, labels, utterances

def write_feature_debug_csv(features, labels, utterances, vectorizer, path, top_n=15):
    if not DEBUG_FEATURE_CSV:
        return
    log_step("Writing feature debug CSV...")
    X = vectorizer.transform(features)
    feature_names = vectorizer.get_feature_names_out()

    totals = np.sum(X, axis=0).A1
    top_indices = np.argsort(totals)[-top_n:][::-1]
    selected = [feature_names[i] for i in top_indices]

    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["utterance", "intent"] + selected)
        for utt, label, row in zip(utterances, labels, X):
            vec = row.toarray().flatten()
            values = [int(vec[feature_names.tolist().index(f)]) if f in feature_names else 0 for f in selected]
            writer.writerow([utt, label] + values)
    print(f"Feature debug file written: {path}")

def summarize_intents(max_count: int, labels: list[str]):
    intent_counts = Counter(label.strip() for label in labels if label.strip())
    print(f"Intent distribution (≤ {max_count} utterances):")
    for intent, count in sorted(intent_counts.items()):
        if count <= max_count:
            print(f"{intent}: {count}")

def cross_val_predict_with_progress(model, X, y, cv_folds):
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    y_pred = np.empty_like(y)
    for train_idx, test_idx in tqdm(skf.split(X, y), total=cv_folds, desc="Cross-validation"):
        model_clone = clone(model)
        model_clone.fit(X[train_idx], y[train_idx])
        y_pred[test_idx] = model_clone.predict(X[test_idx])
    return y_pred

def train_svm_model_with_cv(
    features: list[dict],
    labels: list[str],
    cv_folds: int
) -> tuple[LinearSVC, DictVectorizer, LabelEncoder, list[int], list[int], LabelEncoder]:
    log_step("Starting SVM model training and cross-validation...")

    if ONLY_TRAIN_WITH_INTENTS_THAT_HAVE_AT_LEAST_CV_FOLDS:
        log_step(f"Filtering intents with fewer than {cv_folds} samples...")
        label_counts = Counter(labels)
        retained_indices = [i for i, label in enumerate(labels) if label_counts[label] >= cv_folds]
        if not retained_indices:
            raise ValueError("No labels with enough samples for the given number of CV folds.")
        features = [features[i] for i in retained_indices]
        labels = [labels[i] for i in retained_indices]
        log_step(f"Retained {len(features)} samples after filtering")

    vectorizer = DictVectorizer(sparse=True)
    log_step("Vectorizing feature data...")
    X = vectorizer.fit_transform(features)

    label_encoder = LabelEncoder()
    log_step("Encoding intent labels...")
    y = label_encoder.fit_transform(labels)

    log_step(f"Running {cv_folds}-fold cross-validation...")
    model = LinearSVC(dual=False, max_iter=5000)
    y_pred = cross_val_predict_with_progress(model, X, y, cv_folds)

    log_step("Fitting final model to all data...")
    model.fit(X, y)

    log_step("Training and validation complete.")
    return model, vectorizer, label_encoder, y, y_pred, label_encoder

def report_cross_validation_metrics(
    y_true: list[int],
    y_pred: list[int],
    label_encoder: LabelEncoder,
    report_path: str,
    cv_folds: int
):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
    report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)
    print(f"CV Accuracy: {acc:.4f}")
    print(f"CV Precision: {precision:.4f}")
    print(f"CV Recall: {recall:.4f}")
    print(f"CV F1 Score: {f1:.4f}")

    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"Cross-Validation Report (Folds={cv_folds})\n")
        f.write(f"Accuracy: {acc:.4f}\n")
        f.write(f"Precision: {precision:.4f}\n")
        f.write(f"Recall: {recall:.4f}\n")
        f.write(f"F1 Score: {f1:.4f}\n\n")
        f.write(report)
    print(f"Evaluation report saved to {report_path}")

def write_svm_model_to_file(
    model: svm.SVC,
    vectorizer: DictVectorizer,
    encoder: LabelEncoder,
    path: str
):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'wb') as f:
        pickle.dump({'model': model, 'vectorizer': vectorizer, 'label_encoder': encoder}, f)
    print(f"Model saved to {path}")

def cleanup_training_data_features(input_path: str, output_path: str):
    with open(input_path, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    header = lines[0]
    rows = [line for line in lines[1:] if line.count('|') == 2 and '=' in line]
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(header + '\n')
        for row in rows:
            f.write(row + '\n')
    print(f"Cleansed training data written to {output_path}")

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--cv_folds', type=int, default=DEFAULT_CV_FOLDS, help='Number of cross-validation folds')
    return parser.parse_args()

def main():
    args = parse_args()
    if not os.path.exists(CLEANSED_DATA_PATH):
        print("Cleansed training file not found – cleansing raw features...")
        cleanup_training_data_features(TRAINING_DATA_PATH, CLEANSED_DATA_PATH)

    print("Loading cleansed training data...")
    features, labels, utterances = load_training_data(CLEANSED_DATA_PATH, TRAINING_DATA_HEADER)
    summarize_intents(5, labels)
    print(f"Loaded {len(features)} training samples")

    model, vectorizer, encoder, y_true, y_pred, label_encoder = train_svm_model_with_cv(features, labels, args.cv_folds)
    report_cross_validation_metrics(y_true, y_pred, label_encoder, MODEL_EVAL_REPORT_PATH, args.cv_folds)
    write_svm_model_to_file(model, vectorizer, encoder, MODEL_OUTPUT_PATH)
    write_feature_debug_csv(features, labels, utterances, vectorizer, FEATURE_DEBUG_CSV)

if __name__ == "__main__":
    main()