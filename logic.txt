# svm_trainer.py

from __future__ import annotations
import pickle
import argparse
from datetime import datetime
from collections import Counter
from sklearn import svm
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import cross_val_predict
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report

TRAINING_ROOT_DIR = "/tmp/svm/training/"
TRAINING_DATA_PATH = TRAINING_ROOT_DIR + "features.txt"
CLEANSED_DATA_PATH = TRAINING_ROOT_DIR + "features_cleansed.txt"
MODEL_OUTPUT_PATH = TRAINING_ROOT_DIR + "svm.model"
MODEL_EVAL_REPORT_PATH = TRAINING_ROOT_DIR + "svm_eval_report.txt"

TRAINING_DATA_HEADER = "utterance | features | intent"
DEFAULT_CV_FOLDS = 5

def log_step(message: str):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

def cleanse_feature_string(feature_str: str) -> str:
    feature_str = feature_str.strip()
    prefixes = ["BOW=", "SHORT_BOW=", "CONCEPT=", "SHORT_CONCEPT="]
    cleaned = ""
    i = 0
    first = True
    while i < len(feature_str):
        matched = False
        for prefix in prefixes:
            if feature_str.startswith(prefix, i):
                if not first:
                    if not cleaned.endswith('%'):
                        cleaned += "%"
                cleaned += prefix
                i += len(prefix)
                matched = True
                first = False
                break
        if not matched:
            cleaned += feature_str[i]
            i += 1
    return cleaned.strip()

from collections import Counter

def cleanup_training_data_features(input_filepath: str, output_filepath: str, min_count: int = 5):
    all_rows = []
    with open(input_filepath, encoding='utf-8') as fin:
        lines = [line.rstrip('\n') for line in fin if line.strip()]
        header = lines[0].strip()
        for idx, line in enumerate(lines[1:], start=2):
            parts = [x.strip() for x in line.split('|')]
            if len(parts) != 3:
                print(f"Skipping malformed row on line {idx}: {line}")
                continue
            utterance = parts[0].strip()
            features = parts[1].strip()
            intent = parts[2].strip()
            cleaned_features = cleanse_feature_string(features)
            if not cleaned_features:
                print(f"Skipping row with empty features after cleansing on line {idx}")
                continue
            if cleaned_features != features:
                print(f"Line {idx}: changed'{features}' to '{cleaned_features}'")
            all_rows.append((utterance, cleaned_features, intent))

    intent_counts = Counter(intent for _, _, intent in all_rows)
    retained = [row for row in all_rows if intent_counts[row[2]] >= min_count]
    excluded_by_intent = len(all_rows) - len(retained)

    with open(output_filepath, 'w', encoding='utf-8') as fout:
        fout.write(header + '\n')
        for utterance, features, intent in retained:
            fout.write(f"{utterance} | {features} | {intent}\n")

    print(f"Number of Lines Cleansed: {len(all_rows)}")
    print(f"Excluded {excluded_by_intent} lines with intent count < {min_count}")

def load_training_data(path: str, header: str) -> tuple[list[dict], list[str]]:
    with open(path, encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    assert lines[0] == header
    features = []
    labels = []
    for idx, line in enumerate(lines[1:], start=2):
        try:
            parts = [x.strip() for x in line.split('|')]
            if len(parts) != 3:
                print(f"Malformed row on line {idx}: {line}")
                continue
            feats = dict(pair.split('=') for pair in parts[1].split('%') if '=' in pair)
            features.append(feats)
            labels.append(parts[2])
        except Exception as e:
            print(f"Error processing line {idx}: {line} – {e}")
    return features, labels

def summarize_intents(max_count: int, labels: list[str]):
    intent_counts = Counter(label.strip() for label in labels if label.strip())
    print(f"Intent distribution (≤ {max_count} utterances):")
    for intent, count in sorted(intent_counts.items()):
        if count <= max_count:
            print(f"{intent}: {count}")

def train_svm_model_with_cv(
    features: list[dict],
    labels: list[str],
    cv_folds: int
) -> tuple[svm.SVC, DictVectorizer, LabelEncoder, list[int], list[int], LabelEncoder]:
    log_step("Starting SVM model training and cross-validation...")

    vectorizer = DictVectorizer(sparse=False)
    log_step("Vectorizing feature data...")
    X = vectorizer.fit_transform(features)

    label_encoder = LabelEncoder()
    log_step("Encoding intent labels...")
    y = label_encoder.fit_transform(labels)

    model = svm.SVC(probability=True)
    log_step(f"Running {cv_folds}-fold cross-validation...")
    y_pred = cross_val_predict(model, X, y, cv=cv_folds)

    log_step("Fitting final model to all data...")
    model.fit(X, y)

    log_step("Training and validation complete.")
    return model, vectorizer, label_encoder, y, y_pred, label_encoder

def report_cross_validation_metrics(
    y_true: list[int],
    y_pred: list[int],
    label_encoder: LabelEncoder,
    report_path: str,
    cv_folds: int
):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
    report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)
    print(f"CV Accuracy: {acc:.4f}")
    print(f"CV Precision: {precision:.4f}")
    print(f"CV Recall: {recall:.4f}")
    print(f"CV F1 Score: {f1:.4f}")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"Cross-Validation Report (Folds={cv_folds})\n")
        f.write(f"Accuracy: {acc:.4f}\n")
        f.write(f"Precision: {precision:.4f}\n")
        f.write(f"Recall: {recall:.4f}\n")
        f.write(f"F1 Score: {f1:.4f}\n\n")
        f.write(report)
    print(f"Evaluation report saved to {report_path}")

def write_svm_model_to_file(
    model: svm.SVC,
    vectorizer: DictVectorizer,
    encoder: LabelEncoder,
    path: str
):
    with open(path, 'wb') as f:
        pickle.dump({'model': model, 'vectorizer': vectorizer, 'label_encoder': encoder}, f)
    print(f"Model saved to {path}")

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument('--cv_folds', type=int, default=DEFAULT_CV_FOLDS, help='Number of cross-validation folds')
    return parser.parse_args()

def main():
    args = parse_args()
    training_data_path = TRAINING_DATA_PATH
    training_data_header = TRAINING_DATA_HEADER
    cleansed_data_path = CLEANSED_DATA_PATH
    model_output_path = MODEL_OUTPUT_PATH
    model_eval_report_path = MODEL_EVAL_REPORT_PATH
    cv_folds = args.cv_folds

    print(f"about to read and cleanse features file...")
    features, labels = load_training_data(training_data_path, training_data_header)
    features, labels = load_training_data(cleansed_data_path, training_data_header)
    summarize_intents(5, labels)

    print(f"Loaded {len(features)} training samples")
    model, vectorizer, encoder, y_true, y_pred, label_encoder = train_svm_model_with_cv(features, labels, cv_folds)
    report_cross_validation_metrics(y_true, y_pred, label_encoder, model_eval_report_path, cv_folds)
    write_svm_model_to_file(model, vectorizer, encoder, model_output_path)

def main2():
    print(f"about to read and cleanse features file...")
    cleanup_training_data_features(TRAINING_DATA_PATH, CLEANSED_DATA_PATH)
    features, labels = load_training_data(CLEANSED_DATA_PATH, TRAINING_DATA_HEADER)
    summarize_intents(5, labels)

if __name__ == "__main__":
    main()