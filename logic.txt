# wer_sdi_visualization.py
# convert txt version of WER SID alignment generated using jiwer into html

# multi_model_v1.py

import csv
from pathlib import Path
import os
from collections import defaultdict

# define input and output directories
ALIGNMENT_DIR = Path("j:/projects/sdi/txt/")
HTML_DIR = Path("j:/projects/sdi/html/")

# styles
CORRECT_BKGND_COLOR = "#73f097"
SUBSTITUTION_BKGND_COLOR = "#fafc5c"
INSERTION_BKGND_COLOR = "#55a0e0"
DELETION_BKGND_COLOR = "#ff0000"
WHITE_BKGND_COLOR = "#ffffff"
BLACK_BKGND_COLOR = "#000000"

def get_outcome_style(outcome):
  outcome = outcome.upper() if outcome else ""

  if outcome == "CORRECT":
    return CORRECT_BKGND_COLOR, "#000000", "C"
  elif outcome == "SUBSTITUTION":
    return SUBSTITUTION_BKGND_COLOR, "#000000", "S"
  elif outcome == "DELETION":
    return DELETION_BKGND_COLOR, "#ffffff", "D"
  elif outcome == "INSERTION":
    return INSERTION_BKGND_COLOR, "#ffffff", "I"
  else:
    return WHITE_BKGND_COLOR, "#000000", outcome[0] if outcome else ""

def calculate_model_stats(rows, model_idx, num_models):
  correct_count = 0
  substitution_count = 0
  deletion_count = 0
  insertion_count = 0
  gt_total = 0

  for row in rows:
    if len(row) > model_idx * 3:
      outcome = row[model_idx * 3].upper() if row[model_idx * 3] else ""
      gt = row[model_idx * 3 + 1] if model_idx * 3 + 1 < len(row) else ""

      if outcome == "CORRECT":
        correct_count += 1
        if gt:  # Only count GT if it exists
          gt_total += 1
      elif outcome == "SUBSTITUTION":
        substitution_count += 1
        if gt:  # Only count GT if it exists
          gt_total += 1
      elif outcome == "DELETION":
        deletion_count += 1
        if gt:  # Only count GT if it exists
          gt_total += 1
      elif outcome == "INSERTION":
        insertion_count += 1

  # WER
  if gt_total > 0:
    wer = 100 * (substitution_count + deletion_count + insertion_count) / gt_total
    wer = min(wer, 100)  # Cap at 100
    wer_formatted = f"{wer:.2f}"
  else:
    wer_formatted = "0.00"

  return {
    'correct': correct_count,
    'substitution': substitution_count,
    'deletion': deletion_count,
    'insertion': insertion_count,
    'gt_total': gt_total,
    'wer': wer_formatted
  }

def get_model_name(header, model_idx, txt_file_path):
    # for multi-model files, use the predicted column name (column index: model_idx * 3 + 2)
    if len(header) > model_idx * 3 + 2:
        return header[model_idx * 3 + 2]

    if len(header) > 2:
        return header[2]

    # this should not happen if the txt file has been properly constructed to have 3 columns per model
    return f"Model {model_idx + 1}"

def convert_to_html(txt_file_path, html_file_path):
  print(f"Processing file: {txt_file_path}")

  with open(txt_file_path, 'r', encoding='utf-8') as file:
    header_line = next(file).strip()
    header = header_line.split('|')
    header = [col.strip() for col in header]

    num_models = len(header) // 3 # 3 columns per model
    if num_models == 0:  # Handle edge case
      num_models = 1

    reader = csv.reader(file, delimiter='|')
    all_rows = [[field.strip() for field in row] for row in reader]

  model_stats = [] # counts and wer (using jiwer 3.0.0)
  for i in range(num_models):
    stats = calculate_model_stats(all_rows, i, num_models)
    model_stats.append(stats)

  model_names = []
  for i in range(num_models):
    model_names.append(get_model_name(header, i, txt_file_path))

  # summary results table
  html_content = "<table style='border-collapse: collapse; margin-bottom: 10px;'>\n"
  html_content += "<tr style='background-color: #f2f2f2;'>\n"
  html_content += "<th style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt; font-weight: bold;'>Model</th>\n"
  html_content += "<th style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt; font-weight: bold;'>WER</th>\n"
  html_content += "<th style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt; font-weight: bold;'>GT</th>\n"
  html_content += "<th style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt; font-weight: bold;'>Substitution</th>\n"
  html_content += "<th style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt; font-weight: bold;'>Insertion</th>\n"
  html_content += "<th style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt; font-weight: bold;'>Deletion</th>\n"
  html_content += "<th style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt; font-weight: bold;'>Correct</th>\n"
  html_content += "</tr>\n"

  for i, model_name in enumerate(model_names):
    stats = model_stats[i]
    html_content += "<tr>\n"
    html_content += f"<td style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt;'>{model_name}</td>\n"
    html_content += f"<td style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt;'>{stats['wer']}</td>\n"
    html_content += f"<td style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt;'>{stats['gt_total']}</td>\n"
    html_content += f"<td style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt;'>{stats['substitution']}</td>\n"
    html_content += f"<td style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt;'>{stats['insertion']}</td>\n"
    html_content += f"<td style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt;'>{stats['deletion']}</td>\n"
    html_content += f"<td style='border: 1px solid #ddd; padding: 4px; font-family: Arial; font-size: 7pt;'>{stats['correct']}</td>\n"
    html_content += "</tr>\n"

  html_content += "</table>\n"

  # main results table header
  html_content += "<table>\n<tbody>\n<tr>"
  for i in range(num_models):
    html_content += f"<th style=\"font-family:Arial;font-size:7pt;border:1px dotted black;font-weight:bold;\">&nbsp;</th>"
    html_content += f"<th style=\"font-family:Arial;font-size:7pt;border:1px dotted black;font-weight:bold;\">GT</th>"
    html_content += f"<th style=\"font-family:Arial;font-size:7pt;border:1px dotted black;font-weight:bold;\">{model_names[i]}</th>"
  html_content += "</tr>\n"

  # main results table rows
  for row in all_rows:
    html_content += "<tr>"
    for model_idx in range(num_models):
      col_idx = model_idx * 3
      if col_idx < len(row):
        outcome = row[col_idx] if row[col_idx] else ""
        gt = row[col_idx + 1] if col_idx + 1 < len(row) else ""
        predicted = row[col_idx + 2] if col_idx + 2 < len(row) else ""
      else:
        outcome = ""
        gt = ""
        predicted = ""

      bg_color, text_color, short_outcome = get_outcome_style(outcome)

      html_content += f"<td style=\"font-family:Arial;font-size:7pt;border:1px dotted black;background-color:{bg_color};color:{text_color};\">{short_outcome}</td>"
      html_content += f"<td style=\"font-family:Arial;font-size:7pt;border:1px dotted black;background-color:{bg_color};color:{text_color};\">{gt}</td>"
      html_content += f"<td style=\"font-family:Arial;font-size:7pt;border:1px dotted black;background-color:{bg_color};color:{text_color};\">{predicted}</td>"
    html_content += "</tr>\n"

  html_content += "</tbody>\n</table>"

  with open(html_file_path, 'w', encoding='utf-8') as html_file:
    html_file.write(html_content)

  print(f"Successfully converted {txt_file_path} to {html_file_path}")
  print(f"Processed {num_models} models with {len(all_rows)} rows")

def main(txt_dir, html_dir):
  html_dir.mkdir(parents=True, exist_ok=True)

  print(f"Starting txt to html conversion from {txt_dir} to {html_dir}")

  txt_files = list(txt_dir.glob("*.txt"))

  if not txt_files:
    print("No .txt files found in the input directory.")
    return

  print(f"Found {len(txt_files)} .txt file(s) to process")

  for i, txt_file in enumerate(txt_files, 1):
    print(f"\nProcessing file {i} of {len(txt_files)}")
    html_file = HTML_DIR / f"{txt_file.stem}.html"

    convert_to_html(txt_file, html_file)

  print(f"\nConversion complete. Processed {len(txt_files)} file(s).")

if __name__ == "__main__":
  main(ALIGNMENT_DIR, HTML_DIR)
#==============================================================

# wer_multiple_models_to_txt_alignments.py

from __future__ import annotations

from pathlib import Path
from openpyxl import load_workbook
from nltk_jiwer_utils import get_jiwer_result

SPREADSHEET_DIR = Path("j:/projects/sheet-logic/test-sheets/")
SPREADSHEET_NAME = "test_multiple_models.xlsx" #"test_4_col.xlsx"
TARGET_WORKSHEET = "multiple_models" #"base-vs-engine"
NUMBER_OF_MODELS = 5  # Global variable to set number of model columns
ALIGNMENT_DIR = Path("j:/projects/sdi/multiple-model-alignments/")

def load_target_rows() -> list[dict[str, str]]:
  path = SPREADSHEET_DIR / SPREADSHEET_NAME
  workbook = load_workbook(path)
  worksheet = workbook[TARGET_WORKSHEET]
  headers = [cell.value for cell in next(worksheet.iter_rows(min_row=1, max_row=1))]
  rows = []
  for row in worksheet.iter_rows(min_row=2, values_only=True):
    row_dict = {headers[i]: str(row[i]) if row[i] is not None else "" for i in range(len(headers))}
    rows.append(row_dict)
  return rows

def get_model_columns(headers: list[str]) -> list[str]:
  """Extract model column names from headers"""
  # First two columns are always ID and GT
  return headers[2:2 + NUMBER_OF_MODELS]

def generate_alignment_path(row_id: str) -> Path:
  return ALIGNMENT_DIR / f"{row_id}.txt"

def write_custom_alignments_to_file(header: str, lines: list[str], file_path: str) -> None:
  """Custom file writer that supports dynamic headers for any number of models"""
  with open(file_path, "w", encoding="utf-8") as f:
    f.write(f"{header}\n")
    for line in lines:
      f.write(f"{line}\n")

def get_row_values(alignment_row: str) -> tuple[str, str, str]:
  """Parse a single alignment row into outcome, gt, hyp components"""
  parts = [p.strip() for p in alignment_row.split("|")]
  if len(parts) != 3:
    return ("", "", "")
  return tuple(parts)

def get_single_model_alignment(gt: str, hyp: str) -> list[tuple[str, str, str]]:
  """Get alignment details for a single model compared to ground truth"""
  jiwer_result = get_jiwer_result(gt, hyp)
  alignment_rows = jiwer_result.get("alignment", [])

  alignment_details = []
  for row in alignment_rows:
    outcome, gt_val, hyp_val = get_row_values(row)
    alignment_details.append((outcome, gt_val, hyp_val))

  return alignment_details

def process_alignment_row(row: dict[str, str], model_columns: list[str]) -> None:
  row_id = row["ID"]
  gt = row["GT"]

  # Get all model hypotheses
  hypotheses = [row[model_col] for model_col in model_columns]

  if len(hypotheses) < 1:
    raise ValueError("At least 1 model required for alignment")

  # Get alignments for each model compared to ground truth
  all_alignments = []
  for hyp in hypotheses:
    alignment = get_single_model_alignment(gt, hyp)
    all_alignments.append(alignment)

  # Create appropriate header based on number of models
  header_parts = []
  for i, model_col in enumerate(model_columns):
    # Use actual model column names from spreadsheet
    model_name = model_col.replace("-", "_")  # Sanitize for column header
    # header_parts.extend([f"outcome{i + 1}", "gt", f"{model_name}"])
    header_parts.extend([f" ", "GT", f"{model_name}"])

  custom_header = " | ".join(header_parts)

  # Combine alignments from all models
  max_lines = max(len(align) for align in all_alignments)
  output_lines = []

  for i in range(max_lines):
    line_parts = []
    for alignment in all_alignments:
      if i < len(alignment):
        outcome, gt_val, hyp_val = alignment[i]
        line_parts.extend([outcome, gt_val, hyp_val])
      else:
        line_parts.extend(["", "", ""])  # Empty fields for missing alignments

    output_lines.append(" | ".join(line_parts))

  # Write the file with custom header
  write_custom_alignments_to_file(custom_header, output_lines, str(generate_alignment_path(row_id)))

def main() -> None:
  ALIGNMENT_DIR.mkdir(parents=True, exist_ok=True)
  rows = load_target_rows()

  # Get headers to identify model columns
  path = SPREADSHEET_DIR / SPREADSHEET_NAME
  workbook = load_workbook(path)
  worksheet = workbook[TARGET_WORKSHEET]
  headers = [cell.value for cell in next(worksheet.iter_rows(min_row=1, max_row=1))]

  model_columns = get_model_columns(headers)

  print(f"Found {len(model_columns)} model columns: {model_columns}")

  for row in rows:
    process_alignment_row(row, model_columns)

if __name__ == "__main__":
  main()

# =============================================================
#bulk_response_for_target

import requests
import pandas as pd
import openpyxl
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

THIS_IS_A_TEST = False
FILE_XLSX = "bulk_test_for_target2.xlsx"
TARGET_SHEET = "target"
URL = "http://localhost:9000/rest/utility?utterance="
CHUNK_SIZE = 100
HEADERS = ["Utterance", "intent", "score", "features"]

def fetch_test_data(utterance):
  last_int = int(utterance.split()[-1])
  rem = last_int % 10
  intent = f"intent_{rem}"
  score = last_int/20
  score = float(f"{score:.2f}")
  features = "feat1, feat2, feat3"
  return {"intent": intent, "score": score, "features": features}

def get_json_data(utterance):
    if THIS_IS_A_TEST:
      return fetch_test_data(utterance)
    response = requests.get(f"{URL}{utterance}", verify=False)
    response.raise_for_status()
    data = response.json()
    for entry in data.get("activityLog", []):
        if "Report for component RESPONSE_BUILDER" in entry.get("message", ""):
            input_data = entry.get("input", {})
            intent_info = input_data.get("intent", {}).get("IntentInfo", [])
            if intent_info:
                first = intent_info[0]
                features = first.get("features", [])
                features_str = "[" + ", ".join(f"'{f}'" for f in features) + "]"
                return {"intent": first.get("intent"), "score": first.get("score"), "features": features_str}
    return {"intent": None, "score": None, "features": None}

def get_last_completed_row(ws):
    max_row = ws.max_row
    for i in range(max_row, 1, -1):
        if all(ws[f"{col}{i}"].value not in [None, ""] for col in "ABCD"):
            return i
    return 1

def append_chunk_to_sheet(ws, df_chunk, start_excel_row):
    for excel_row_idx, df_row in enumerate(df_chunk.itertuples(index=False), start=start_excel_row):
        ws.cell(row=excel_row_idx, column=1, value=df_row.Utterance)
        ws.cell(row=excel_row_idx, column=2, value=df_row.intent)
        ws.cell(row=excel_row_idx, column=3, value=df_row.score)
        ws.cell(row=excel_row_idx, column=4, value=df_row.features)

def format_sheet(ws):
  if THIS_IS_A_TEST:
    ws.column_dimensions["A"].width = 30
    ws.column_dimensions["B"].width = 25
    ws.column_dimensions["C"].width = 25
    ws.column_dimensions["D"].width = 30
  else:
    ws.column_dimensions["A"].width = 100
    ws.column_dimensions["B"].width = 35
    ws.column_dimensions["C"].width = 20
    ws.column_dimensions["D"].width = 150
  for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=4, max_col=4):
      for cell in row:
          cell.alignment = openpyxl.styles.Alignment(wrap_text=True)

def ensure_valid_header(ws):
    values = [ws[f"{col}1"].value for col in "ABCD"]
    if values != HEADERS:
        for col_index, header in enumerate(HEADERS, start=1):
            ws.cell(row=1, column=col_index, value=header)

def main():
  wb = openpyxl.load_workbook(FILE_XLSX)
  if TARGET_SHEET not in wb.sheetnames:
    print(f"Worksheet '{TARGET_SHEET}' not found. Exiting.")
    return

  ws = wb[TARGET_SHEET]
  ensure_valid_header(ws)

  row = 2  # Start from row 2 (skip header)
  chunk = []
  chunk_start_row = None

  while row <= ws.max_row:
    val_a = ws[f"A{row}"].value
    val_b = ws[f"B{row}"].value
    val_c = ws[f"C{row}"].value
    val_d = ws[f"D{row}"].value

    if all(v in [None, ""] for v in (val_a, val_b, val_c, val_d)):
      ws.delete_rows(row)
      continue

    if val_a not in [None, ""] and (val_b in [None, ""] or val_c in [None, ""] or val_d in [None, ""]):
      utterance = str(val_a).strip("[]")
      if utterance.lower() in ("", "nan"):
        row += 1
        continue
      print(f"[{row}] Processing: {utterance}")
      result = get_json_data(utterance)
      chunk.append([utterance, result["intent"], result["score"], result["features"]])
      if chunk_start_row is None:
        chunk_start_row = row
      if len(chunk) == CHUNK_SIZE:
        df_chunk = pd.DataFrame(chunk, columns=HEADERS)
        append_chunk_to_sheet(ws, df_chunk, start_excel_row=chunk_start_row)
        wb.save(FILE_XLSX)
        row = chunk_start_row + CHUNK_SIZE
        chunk = []
        chunk_start_row = None
        continue

    row += 1

  if chunk:
    df_chunk = pd.DataFrame(chunk, columns=HEADERS)
    append_chunk_to_sheet(ws, df_chunk, start_excel_row=chunk_start_row)
    wb.save(FILE_XLSX)

  format_sheet(ws)
  wb.save(FILE_XLSX)

if __name__ == "__main__":
    main()