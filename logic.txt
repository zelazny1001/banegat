===FILE_START===:business_inputs_to_monthly.py
#business_inputs_to_monthly.py

from __future__ import annotations
import os
from openpyxl import load_workbook, Workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font
from constants import INPUT_DATA_WORKSHEET_NAME, WORKSHEET_PREFIX

def add_input_data_worksheet_with_metadata(monthly_workbook: Workbook, metrics_workbook_paths: list[str]) -> None:
    consolidated_input_worksheet  = monthly_workbook.create_sheet(INPUT_DATA_WORKSHEET_NAME, 0)
    header_written = True
    for metrics_path in metrics_workbook_paths:
        metrics_workbook = load_workbook(metrics_path)
        input_sheet_name = next(name for name in metrics_workbook.sheetnames if name.startswith(WORKSHEET_PREFIX))
        input_worksheet = metrics_workbook[input_sheet_name]
        metadata_value = os.path.splitext(os.path.basename(metrics_path))[0]
        for row_index, row_values in enumerate(input_worksheet.iter_rows(values_only=True), start=1):
            if row_index == 1:
                if header_written:
                    consolidated_input_worksheet.append(["metadata"] + list(row_values))
                    header_written = False
                continue
            if not any((str(cell).strip() if cell is not None else "") for cell in row_values):
                continue
            consolidated_input_worksheet.append([metadata_value] + list(row_values))
    consolidated_input_worksheet.freeze_panes = "A2"
    consolidated_input_worksheet.auto_filter.ref = f"A1:{get_column_letter(consolidated_input_worksheet.max_column)}{consolidated_input_worksheet .max_row}"
    for col in range(1, consolidated_input_worksheet.max_column + 1):
        consolidated_input_worksheet.column_dimensions[get_column_letter(col)].width = 35
    for cell in consolidated_input_worksheet [1]:
        cell.font = Font(name="Aptos", size=9, bold=True)
===FILE_END===:business_inputs_to_monthly.py
===FILE_START===:constants.py
# constants.py

from __future__ import annotations

USE_HALLUCINATION_FLAGS: bool = False
CODE_IS_BEING_TESTED: bool = False
INCLUDE_CONDITIONS = False

ROOT_DIR: str = "j:/projects/sheet-logic/asr-april-2025-data-agent-cust2/" # asr-april-2025-data"
SPREADSHEET_PREFIX: str = "ASR"
WORKSHEET_PREFIX: str = "ASR"
METRICS_CONDITION_TEMPLATE = "metrics-condition-template.xlsx"
DO_MONTHLY_CONSOLIDATION: bool = True
CALL_LEVEL_GRANULARITY: bool = True # False means sample level granularity
AVERAGING_PERIOD =  "weekly" # "monthly"

SESSION_ID_COL_NAME: str = "session_id"
RAW_TRANSCRIPT_COL_NAME: str = "raw_transcript"
GROUND_TRUTH_COL_NAME: str = "Ground Truth Transcript"
HALLUCINATION_COL_NAME: str = "Hallucination"

#=== start modification for model_toks - constants.py
MODEL_TOKS_COL_NAME: str = "Model Toks"
#=== end modification for model_toks

#METRICS_HEADER_COL_WIDTHS = {"A": 27, "B": 65, "C": 9, "D": 63, "E": 63, "F": 8, "G": 12, "H": 10, "I": 10, "J": 15, "K": 12, "L": 12}
#=== start modification for timestamp - constants.py
TIMESTAMP_COL_NAME: str = "Timestamp"
DURATION_COL_NAME: str = "duration"
REQS_COL_NAME: str = "Reqs"
TRANSCRIPT_DENSITY_COL_NAME: str = "Reqs/Min."
AVERAGE_TRX_DENSITY_COL_NAME: str = "Avg. Reqs/Min."
#=== end modification for timestamp

INPUT_DATA_WORKSHEET_NAME: str = "input-data"

METRICS_WORKSHEET_NAME: str = "metrics"
AGENT_CUSTOMER_METRICS_WORKSHEET_NAME: str = "agent-customer-metrics"
METADATA_COL_NAME: str = "metadata"
SECTION_COL_NAME: str = "section"
ENTIRE_COL_NAME: str = "entire"
GT_SECTION_TEXT_COL_NAME: str = "gt section"
MODEL_SECTION_TEXT_COL_NAME: str = "model text"
WER_COL_NAME: str = "WER%"
HALLUCINATION_COUNT_COL_NAME: str = "Hallucination Count"
GT_TOKS_COL_NAME: str = "GT Toks"
HALLUCINATION_PERCENT_COL_NAME: str = "Hallucination %"

NUMBER_OF_SECTIONS: int = 5
WER_POST_ENDPOINT: str = "https://wer_host:3281/word_error_rate"
WER_ALGORITHM = "jiwer"
COMPUTE_WER_LOCAL = True

WEEKLY_SUMMARY_WORKSHEET_NAME: str = "weekly-summary"
MONTHLY_SUMMARY_WORKSHEET_NAME: str = "monthly-summary"

AVERAGE_WER_COL_NAME: str = "AVG_WER"
SESSION_COUNT_COL_NAME: str = "num sessions"
SUMMARY_HALLUCINATION_COUNT_COL_NAME: str = "Hallucination Count"
HALLUCINATION_AVG_COL_NAME: str = "Hallucination Avg"
AVG_HALLUCINATION_PERCENT_COL_NAME: str = "Avg Hallucination %"

CALL_LEVEL_BKGND_COLOR: str = "00FFFF"
SAMPLE_LEVEL_BKGND_COLOR: str = "FFFFFF"
SOURCE_COL_WIDTHS = {"A": 69, "B": 35, "C": 35, "D": 12}
#METRICS_HEADER_COL_WIDTHS = {"A": 27, "B": 65, "C": 9, "D": 63, "E": 63, "F": 8, "G": 12, "H": 10, "I": 15}
#SUMMARY_COL_WIDTHS = {"A": 30, "B": 13, "C": 16, "D": 22}
#=== start modification for timestamp - constants.py
#METRICS_HEADER_COL_WIDTHS = {"A": 27, "B": 65, "C": 9, "D": 63, "E": 63, "F": 8, "G": 12, "H": 10, "I": 15, "J": 12, "K": 12}
#SUMMARY_COL_WIDTHS = {"A": 30, "B": 13, "C": 16, "D": 22, "E": 15}
#=== end modification for timestamp

# Agent / Customer interaction support
VTYPE_COL_NAME: str = "vtype"
AGENT_VALUE: str = "Agent"
CUSTOMER_VALUE: str = "Customer"
BOTH_VALUE: str = "Both"

#=== start modification for toxic columns - constants.py
TOXIC_WORKBOOK_PATH: str = "j:/projects/sheet-logic/toxic_words.csv"
TOXIC_COL_NAME: str = "Toxic"
TOXIC_PERCENT_COL_NAME: str = "Toxic%"
#=== end modification for toxic columns

#=== start modification for S, I, D columns - constants.py
S_COL_NAME: str = "S"
I_COL_NAME: str = "I"
D_COL_NAME: str = "D"
#=== end modification for S, I, D columns

#=== start modification for toxic columns - constants.py
METRICS_HEADER_COL_WIDTHS = {"A": 27, "B": 40, "C": 9, "D": 63, "E": 63, "F": 8, "G": 6, "H": 6, "I": 6, "J": 8, "K": 10, "L": 10, "M": 12, "N": 12, "O": 12, "P":8, "Q":8, "R":7, "S":10}
#=== end modification for toxic columns

#=== start modification for summary toxic% - constants.py
AVG_TOXIC_PERCENT_COL_NAME: str = "Toxic%"
#SUMMARY_COL_WIDTHS = {"A": 30, "B": 13, "C": 16, "D": 22, "E": 15, "F": 15}
SUMMARY_COL_WIDTHS = {"A": 30, "B": 13, "C": 16, "D": 22, "E": 12, "F": 15, "G": 15}
#=== end modification for summary toxic%
===FILE_END===:constants.py
===FILE_START===:data_models.py
# data_models.py

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Optional

#=== start modification for timestamp - data_models.py
@dataclass
class TranscriptPair:
    transcript: str
    ground_truth: str
    timestamp: Optional[int] = None
    duration: int = 0
#=== end modification for timestamp

@dataclass
class HallucinationData:
    counts: list[int]
    texts: list[str]

@dataclass
class SessionData:
    session_id: str
    transcript_pairs: list[TranscriptPair]
    hallucination_data: Optional[HallucinationData] = None

@dataclass
class SpeakerSessionData:
    session_id: str
    customer_pairs: list[TranscriptPair] = field(default_factory=list)
    agent_pairs: list[TranscriptPair] = field(default_factory=list)
    customer_hallucinations: Optional[HallucinationData] = None
    agent_hallucinations: Optional[HallucinationData] = None

#=== start modification for S, I, D columns - data_models.py
@dataclass
class MetricsRow:
    metadata: str
    session_id: str
    section: str
    ground_truth_text: str
    model_text: str
    wer: float
    hallucination_count: int
    ground_truth_tokens: int
    model_tokens: int
    hallucination_percent: Optional[float]
    hallucination_text: str
    toxic_text: str
    toxic_percent: Optional[float]
    substitutions: int = 0
    insertions: int = 0
    deletions: int = 0
    duration: int = 0
    row_count: int = 0
    transcript_density: Optional[float] = None
    is_entire: bool = False
#=== end modification for S, I, D columns

#=== start modification for summary toxic% - data_models.py
@dataclass
class SummaryData:
    metadata: str
    average_wer: float
    session_count: int
    average_hallucination_percent: float
    reqs: int
    speaker: Optional[str] = None
    average_transcript_density: Optional[float] = None
    toxic_percent: Optional[float] = None
#=== end modification for summary toxic%
===FILE_END===:data_models.py
===FILE_START===:file_manager.py
# file_manager.py

from __future__ import annotations
import os
from typing import Optional
from openpyxl import load_workbook
from constants import SPREADSHEET_PREFIX, WORKSHEET_PREFIX

class FileManager:
  def __init__(self, root_dir: str):
    self.root_dir = root_dir

  def list_input_spreadsheets(self) -> list[str]:
    return sorted(
      os.path.join(self.root_dir, name)
      for name in os.listdir(self.root_dir)
      if name.startswith(f"{SPREADSHEET_PREFIX}_")
      and name.lower().endswith(".xlsx")
      and not name.endswith("_metrics.xlsx")
      and not name.endswith("_monthly_metrics.xlsx")
    )

  def qualifies_for_processing(self, input_path: str) -> tuple[bool, Optional[str]]:
    try:
      workbook = load_workbook(input_path, read_only=True, data_only=True, keep_links=False)
      try:
        worksheet_name = next(
          (n for n in workbook.sheetnames if n.startswith(WORKSHEET_PREFIX)),
          None
        )
        return worksheet_name is not None, worksheet_name
      finally:
        workbook.close()
    except Exception:
      return False, None

  def get_metrics_output_path(self, input_path: str) -> str:
    directory = os.path.dirname(input_path)
    base = os.path.basename(input_path).rsplit(".", 1)[0]
    return os.path.join(directory, f"{base}_metrics.xlsx")
===FILE_END===:file_manager.py
===FILE_START===:main_orchestrator.py
# main_orchestrator.py

from __future__ import annotations
from file_manager import FileManager
from workbook_processor import WorkbookProcessor
from monthly_workbook_processor import MonthlyWorkbookProcessor
from constants import ROOT_DIR, SPREADSHEET_PREFIX, DO_MONTHLY_CONSOLIDATION

class ApplicationOrchestrator:
  def __init__(self, root_dir: str):
    self.root_dir = root_dir
    self.file_manager = FileManager(root_dir)
    self.workbook_processor = WorkbookProcessor()
    self.monthly_processor = MonthlyWorkbookProcessor()

  def run(self) -> None:
    metrics_files = self._process_input_files()

    if DO_MONTHLY_CONSOLIDATION and metrics_files:
      self._create_monthly_consolidation(metrics_files)

  def _process_input_files(self) -> list[str]:
    input_files = self.file_manager.list_input_spreadsheets()
    metrics_files: list[str] = []

    for index, input_path in enumerate(input_files, start=1):
      ok, worksheet_name = self.file_manager.qualifies_for_processing(input_path)

      if not ok:
        print(f"{index}/{len(input_files)} Skipping {input_path}: no worksheet starting with {SPREADSHEET_PREFIX}")
        continue

      output_path = self.workbook_processor.process_input_workbook(input_path)

      if output_path:
        metrics_files.append(output_path)
        print(f"{index}/{len(input_files)} Saved metrics and summaries to {output_path}")

    return metrics_files

  def _create_monthly_consolidation(self, metrics_files: list[str]) -> None:
    monthly_path, monthly_workbook = self.monthly_processor.create_monthly_workbook(
      self.root_dir,
      metrics_files,
      SPREADSHEET_PREFIX
    )
    print(f"Saved monthly consolidated metrics to {monthly_path}")

def main() -> None:
  orchestrator = ApplicationOrchestrator(ROOT_DIR)
  orchestrator.run()

if __name__ == "__main__":
  main()
===FILE_END===:main_orchestrator.py
===FILE_START===:metrics_calculator.py
# metrics_calculator.py

from __future__ import annotations
import os
import re
import random
import requests
from typing import Tuple
from data_models import TranscriptPair, MetricsRow
from constants import (CODE_IS_BEING_TESTED,
                       WER_POST_ENDPOINT,
                       COMPUTE_WER_LOCAL,
                       WER_ALGORITHM,
                       NUMBER_OF_SECTIONS,
                       TOXIC_WORKBOOK_PATH)
from nltk_jiwer_utils import get_nltk_result, get_jiwer_result

class TextPreprocessor:
  @staticmethod
  def preprocess(text: str) -> str:
    if not text:
      return ""
    lowered = str(text).lower()
    lowered = re.sub(r"[.,\-?â€¦]", " ", lowered)
    lowered = re.sub(r"\{[^}]*\}", "", lowered)
    lowered = re.sub(r"[\[\]]", "", lowered)
    return re.sub(r"\s+", " ", lowered).strip()

class WERCalculator:
  def __init__(self, endpoint: str):
    self.endpoint = endpoint
    self.preprocessor = TextPreprocessor()
    self.cached_result = None
    print(f"WERCalculator started ...")

  # === start modification for S, I, D columns - metrics_calculator.py (WERCalculator.calculate)
  def calculate(self, ground_truth: str, transcript: str) -> Tuple[float, int, int, int, int, int]:
    if CODE_IS_BEING_TESTED:
      return round(random.uniform(40, 90), 4), len(ground_truth.split()), len(transcript.split()), 0, 0, 0

    if COMPUTE_WER_LOCAL:
      if WER_ALGORITHM == "nltk":
        self.cached_result = get_nltk_result(ground_truth, transcript)
      else:
        self.cached_result = get_jiwer_result(ground_truth, transcript)

      wer = self.cached_result["wer"]
      references = self.cached_result["references"]
      hypothesis = self.cached_result["hypothesis"]
      substitutions = self.cached_result.get("substitutions", 0)
      insertions = self.cached_result.get("insertions", 0)
      deletions = self.cached_result.get("deletions", 0)
      # Handle empty string values from nltk
      if substitutions == "":
        substitutions = 0
      if insertions == "":
        insertions = 0
      if deletions == "":
        deletions = 0
      return wer, references, hypothesis, substitutions, insertions, deletions
  # === end modification for S, I, D columns

    # try:
    #   response = requests.post(
    #     self.endpoint,
    #     json={"groundTruth": ground_truth, "transcript": transcript},
    #     verify=False,
    #     timeout=10
    #   )
    #   data = response.json()
    #   return round(float(data[0]), 4), int(data[1])
    # except Exception:
    #   return float("nan"), None

  # === start modification for S, I, D columns - metrics_calculator.py (calculate_for_pairs)
  def calculate_for_pairs(self, pairs: list[TranscriptPair]) -> Tuple[str, str, float, int, int, int, int]:
    ground_truth_joined = " ".join(
      self.preprocessor.preprocess(p.ground_truth) for p in pairs
    )
    transcript_joined = " ".join(
      self.preprocessor.preprocess(p.transcript) for p in pairs
    )
    wer, tokens, model_tokens, substitutions, insertions, deletions = self.calculate(ground_truth_joined, transcript_joined)
    return ground_truth_joined, transcript_joined, wer, tokens, substitutions, insertions, deletions
  # === end modification for S, I, D columns

class SectionSplitter:
  @staticmethod
  def compute_spans(total_items: int, section_count: int) -> list[tuple[int, int]]:
    if total_items == 0 or section_count <= 0:
      return []

    base_size = total_items // section_count
    remainder = total_items % section_count
    spans: list[tuple[int, int]] = []
    start = 0

    for i in range(1, section_count + 1):
      size = base_size + (1 if i <= remainder else 0)
      if size == 0:
        break
      end = start + size
      spans.append((start, end))
      start = end

    return spans

class MetricsCalculator:
  # === start modification for toxic columns - metrics_calculator.py
  def __init__(self):
    self.wer_calculator = WERCalculator(WER_POST_ENDPOINT)
    self.section_splitter = SectionSplitter()
    self.toxic_detector = ToxicTokenDetector(TOXIC_WORKBOOK_PATH)
  # === end modification for toxic columns

  # === start modification for S, I, D columns - metrics_calculator.py (calculate_metrics_for_section)
  def calculate_metrics_for_section(
      self,
      metadata: str,
      session_id: str,
      section_label: str,
      pairs: list[TranscriptPair],
      hallucination_counts: list[int],
      hallucination_texts: list[str],
      is_entire: bool = False
  ) -> MetricsRow:
    if not pairs:
      return MetricsRow(
        metadata=metadata,
        session_id=session_id,
        section=section_label,
        ground_truth_text="",
        model_text="",
        wer=float("nan"),
        hallucination_count=0,
        ground_truth_tokens=0,
        model_tokens=0,
        hallucination_percent=None,
        hallucination_text="",
        toxic_text="",
        toxic_percent=None,
        substitutions=0,
        insertions=0,
        deletions=0,
        duration=0,
        row_count=0,
        transcript_density=None,
        is_entire=is_entire
      )

    gt_text, model_text, wer, gt_tokens, substitutions, insertions, deletions = self.wer_calculator.calculate_for_pairs(pairs)
    hallucination_total = sum(hallucination_counts)

    total_duration = sum(p.duration for p in pairs)
    model_tokens = len(model_text.split()) if model_text else 0

    hallucination_percent = (100 * hallucination_total / model_tokens) if model_tokens else None
    hallucination_text = " ".join(hallucination_texts)

    row_count = len(pairs)
    transcript_density = (60 * 1 * row_count) / total_duration if total_duration > 0 else None

    # Find toxic tokens in model text
    toxic_tokens_found, toxic_count, toxic_percent = self.toxic_detector.find_toxic_tokens(model_text)
    toxic_text = ", ".join(toxic_tokens_found) if toxic_tokens_found else ""

    return MetricsRow(
      metadata=metadata,
      session_id=session_id,
      section=section_label,
      ground_truth_text=gt_text,
      model_text=model_text,
      wer=wer,
      hallucination_count=hallucination_total,
      ground_truth_tokens=gt_tokens,
      model_tokens=model_tokens,
      hallucination_percent=hallucination_percent,
      hallucination_text=hallucination_text,
      toxic_text=toxic_text,
      toxic_percent=toxic_percent,
      substitutions=substitutions,
      insertions=insertions,
      deletions=deletions,
      duration=total_duration,
      row_count=row_count,
      transcript_density=transcript_density,
      is_entire=is_entire
    )

  # === end modification for S, I, D columns

  def calculate_section_metrics(
      self,
      metadata: str,
      session_id: str,
      pairs: list[TranscriptPair],
      hallucination_counts: list[int],
      hallucination_texts: list[str]
  ) -> list[MetricsRow]:
    spans = self.section_splitter.compute_spans(len(pairs), NUMBER_OF_SECTIONS)
    metrics: list[MetricsRow] = []

    for section_number, (start, end) in enumerate(spans, start=1):
      section_pairs = pairs[start:end]
      section_halluc_counts = hallucination_counts[start:end]
      section_halluc_texts = hallucination_texts[start:end]

      metrics.append(
        self.calculate_metrics_for_section(
          metadata,
          session_id,
          section_number,
          section_pairs,
          section_halluc_counts,
          section_halluc_texts,
          is_entire=False
        )
      )

    return metrics

# === start modification for toxic columns - metrics_calculator.py
class ToxicTokenDetector:
  def __init__(self, toxic_file_path: str):
    self.toxic_tokens = self._load_toxic_tokens(toxic_file_path)

  def _load_toxic_tokens(self, file_path: str) -> set[str]:
    """Load toxic tokens from file, one token per line or comma-separated."""
    toxic_tokens = set()
    if not os.path.exists(file_path):
      return toxic_tokens

    try:
      with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
        # Support both newline-separated and comma-separated formats
        tokens = content.replace('\n', ',').split(',')
        toxic_tokens = {token.strip().lower() for token in tokens if token.strip()}
    except Exception as e:
      print(f"Warning: Could not load toxic tokens from {file_path}: {e}")

    return toxic_tokens

  def find_toxic_tokens(self, text: str) -> tuple[list[str], int, float]:
    """
    Find toxic tokens in text.
    Returns: (list of toxic tokens found, count, percentage)
    """
    if not text:
      return [], 0, None

    # Split text into tokens (words)
    tokens = text.lower().split()
    total_tokens = len(tokens)

    if total_tokens == 0:
      return [], 0, None

    # Find toxic tokens
    toxic_found = [token for token in tokens if token in self.toxic_tokens]
    toxic_count = len(toxic_found)
    toxic_percent = round((100 * toxic_count / total_tokens), 2) if total_tokens > 0 else None

    return toxic_found, toxic_count, toxic_percent
# === end modification for toxic columns
===FILE_END===:metrics_calculator.py
===FILE_START===:monthly_workbook_processor.py
# monthly_workbook_processor.py

from __future__ import annotations
import os
import re
from datetime import datetime
from openpyxl import load_workbook, Workbook
from openpyxl.styles import Font, Alignment
from openpyxl.utils import get_column_letter
from worksheet_reader import WorksheetReader
from summary_calculator import SummaryCalculator
from worksheet_writer import SummaryWorksheetWriter
from constants import (
  WORKSHEET_PREFIX,
  METRICS_WORKSHEET_NAME,
  AGENT_CUSTOMER_METRICS_WORKSHEET_NAME,
  INPUT_DATA_WORKSHEET_NAME,
  MONTHLY_SUMMARY_WORKSHEET_NAME,
  METRICS_HEADER_COL_WIDTHS,
)

class MonthlyWorkbookProcessor:
  def __init__(self):
    self.summary_calculator = SummaryCalculator()

  def create_monthly_workbook(
      self,
      root_dir: str,
      metrics_paths: list[str],
      prefix: str
  ) -> tuple[str, Workbook]:
    monthly_path = self._get_monthly_output_path(root_dir, metrics_paths, prefix)

    if os.path.exists(monthly_path):
      os.remove(monthly_path)

    has_vtype = self._check_vtype_presence(metrics_paths)

    monthly_workbook = Workbook()
    del monthly_workbook[monthly_workbook.sheetnames[0]]

    self._add_consolidated_input_data(monthly_workbook, metrics_paths)
    self._add_consolidated_metrics(monthly_workbook, metrics_paths, has_vtype)
    self._add_monthly_summary(monthly_workbook, has_vtype)

    monthly_workbook.save(monthly_path)
    return monthly_path, monthly_workbook

  def _get_monthly_output_path(self, root_dir: str, metrics_paths: list[str], prefix: str) -> str:
    dates = []

    for metrics_path in metrics_paths:
      if '_week_of_' in metrics_path:
        _, start, end, year = self._parse_weekly_format(metrics_path)
        dates.append((start, end, year))
      else:
        base = os.path.basename(metrics_path).rsplit(".", 1)[0]
        parts = base.split("_")
        rng, year = parts[1], parts[2]
        start, end = re.split(r"thru|-", rng, maxsplit=1)
        dates.append((start, end, year))

    earliest = min(d[0] for d in dates)
    latest = max(d[1] for d in dates)
    year = dates[0][2]
    filename = f"{prefix}_{earliest}thru{latest}_{year}_monthly_metrics.xlsx"
    return os.path.join(root_dir, filename)

  def _parse_weekly_format(self, path: str) -> tuple[str, str, str, str]:
    earliest = path[-17:-13]
    latest = path[-17:-13]
    year = str(datetime.now().year)
    return path, earliest, latest, year

  def _check_vtype_presence(self, metrics_paths: list[str]) -> bool:
    for path in metrics_paths:
      try:
        workbook = load_workbook(path, read_only=True, data_only=True, keep_links=False)
        try:
          worksheet_name = next((n for n in workbook.sheetnames if n.startswith(WORKSHEET_PREFIX)), None)
          if worksheet_name:
            worksheet = workbook[worksheet_name]
            reader = WorksheetReader(worksheet)
            if reader.has_vtype_column():
              return True
        finally:
          workbook.close()
      except Exception:
        continue
    return False

  def _add_consolidated_input_data(self, monthly_workbook: Workbook, metrics_paths: list[str]) -> None:
    from business_inputs_to_monthly import add_input_data_worksheet_with_metadata
    add_input_data_worksheet_with_metadata(monthly_workbook, metrics_paths)

    from speaker_metrics_styling import style_input_data_worksheet
    input_data_ws = monthly_workbook[monthly_workbook.sheetnames[0]]
    style_input_data_worksheet(input_data_ws)

  def _add_consolidated_metrics(
      self,
      monthly_workbook: Workbook,
      metrics_paths: list[str],
      has_vtype: bool
  ) -> None:
    sheet_name = AGENT_CUSTOMER_METRICS_WORKSHEET_NAME if has_vtype else METRICS_WORKSHEET_NAME
    monthly_metrics_ws = monthly_workbook.create_sheet(sheet_name)

    first = True
    for metrics_path in metrics_paths:
      metrics_wb = load_workbook(metrics_path)
      metrics_ws = metrics_wb[METRICS_WORKSHEET_NAME]

      for i, row_values in enumerate(metrics_ws.iter_rows(values_only=True), start=1):
        if i == 1 and not first:
          continue
        monthly_metrics_ws.append(row_values)
      first = False

    from speaker_metrics_styling import apply_agent_customer_metrics_styling
    apply_agent_customer_metrics_styling(monthly_metrics_ws)

  # === start modification for overall toxic% in G4 - monthly_workbook_processor.py
  def _add_monthly_summary(self, monthly_workbook: Workbook, has_vtype: bool) -> None:
    metrics_ws = monthly_workbook[
      AGENT_CUSTOMER_METRICS_WORKSHEET_NAME if has_vtype else METRICS_WORKSHEET_NAME
    ]

    summaries = self.summary_calculator.calculate_overall_summary(metrics_ws)

    writer = SummaryWorksheetWriter(monthly_workbook, MONTHLY_SUMMARY_WORKSHEET_NAME)
    writer.create_worksheet(has_speaker_column=has_vtype)

    for row_num, summary in enumerate(summaries, start=2):
      writer.write_summary_row(summary, row_num, has_speaker_column=has_vtype)

    # Write overall toxic percentage to cell G4 (or F4)
    writer.write_overall_toxic_percent(metrics_ws, has_speaker_column=has_vtype)

    from speaker_metrics_styling import style_monthly_summary_worksheet

    style_monthly_summary_worksheet(writer.worksheet, has_vtype)
  # === end modification for overall toxic% in G4
===FILE_END===:monthly_workbook_processor.py
===FILE_START===:nltk_jiwer_utils.py
# nltk_jiwer_utils.py - library version based on jiwer 3.0.0, can be used by clients

from __future__ import annotations
import unicodedata
import string
from nltk import edit_distance
from jiwer import process_words, compute_measures

PLACES_OF_DECIMAL = 4

def strip_accents(s: str) -> str:
    return ''.join(c for c in unicodedata.normalize('NFD', s)
                   if unicodedata.category(c) != 'Mn' and ord(c) <= 122)

def normalize_string(s: str) -> list[str]:
    s = strip_accents(s.lower().translate(str.maketrans('', '', string.punctuation)))
    tokens = s.split()
    normalized: list[str] = []
    for token in tokens:
        if token == 'okay':
            normalized.append('ok')
        elif token in ['yeah', 'yea', 'ya', 'yep', 'yup', 'yes']:
            normalized.append('yeah')
        elif token == 'cuz':
            normalized.append('because')
        elif token.isupper() or token in ['uh-huh', 'uhuh', 'uh', 'huh', 'um', 'hm', 'umm', 'hmm']:
            continue
        else:
            normalized.append(token)
    return normalized

def get_nltk_result(gt: str, hyp: str) -> dict:
    ref_tokens = normalize_string(gt)
    hyp_tokens = normalize_string(hyp)
    if not ref_tokens:
        wer = 0.0 if not hyp_tokens else 1.0
        return {
            "wer": 100*round(wer, PLACES_OF_DECIMAL),
            "substitutions": 0,
            "deletions": 0,
            "insertions": len(hyp_tokens),
            "references": 0,
            "hypothesis": len(hyp_tokens),
            "alignment": []
        }
    ed = edit_distance(ref_tokens, hyp_tokens)
    return {
        "wer": round(100 * min(1.0, ed / len(ref_tokens)), PLACES_OF_DECIMAL),
        "substitutions": "",
        "deletions": "",
        "insertions": "",
        "references": len(ref_tokens),
        "hypothesis": len(hyp_tokens),
        "alignment": []
    }

def get_jiwer_result(gt: str, hyp: str) -> dict:
    if gt == '' or hyp == '' or gt == ' ' or hyp == ' ' or gt == None or hyp == None:
        return {
            "wer": 200,
            "substitutions": "",
            "deletions": "",
            "insertions": "",
            "references": 0,
            "hypothesis": 0,
            "alignment": []
        }

    ref_tokens = normalize_string(gt)
    hyp_tokens = normalize_string(hyp)
    ref_str = " ".join(ref_tokens)
    hyp_str = " ".join(hyp_tokens)

    measures = compute_measures(ref_str, hyp_str)
    alignment = process_words(ref_str, hyp_str)

    aligned_output = []
    for group in alignment.alignments:
        for chunk in group:
            try:
                op = chunk.type
                ref_segment = ref_tokens[chunk.ref_start_idx:chunk.ref_end_idx]
                hyp_segment = hyp_tokens[chunk.hyp_start_idx:chunk.hyp_end_idx]

                if op == 'equal':
                    for rw, hw in zip(ref_segment, hyp_segment):
                        aligned_output.append(f"CORRECT | {rw} | {hw}")
                elif op == 'substitute':
                    for rw, hw in zip(ref_segment, hyp_segment):
                        aligned_output.append(f"SUBSTITUTION | {rw} | {hw}")
                elif op == 'insert':
                    for hw in hyp_segment:
                        aligned_output.append(f"INSERTION |  | {hw}")
                elif op == 'delete':
                    for rw in ref_segment:
                        aligned_output.append(f"DELETION | {rw} | ")
            except Exception:
                continue

    return {
        "wer": round(100 * min(1.0, measures['wer']), 2), #100*min(1.0, round(measures['wer'], 2)),
        "substitutions": measures['substitutions'],
        "deletions": measures['deletions'],
        "insertions": measures['insertions'],
        "references": len(ref_tokens),
        "hypothesis": len(hyp_tokens),
        "alignment": aligned_output
    }


def get_wer_from_json(result: dict) -> float:
    return round(result.get("wer", 1.0), 2)

def get_gt_tok_count_from_json(result: dict) -> int:
    return result.get("references")

def get_nltk_wer(nltk_result: dict) -> float:
    return round(nltk_result.get("wer", 1.0), 2)

def get_jiwer_alignment(jiwer_result: dict) -> list[str]:
    return jiwer_result.get("alignment", [])

def get_row_values(alignment_row: str) -> tuple[str, str, str]:
    parts = [p.strip() for p in alignment_row.split("|")]
    if len(parts) != 3:
        return ("", "", "")
    return tuple(parts)

def write_alignment_to_file(file_path: str, jiwer_alignment: list[str]) -> None:
    with open(file_path, "w", encoding="utf-8") as f:
        f.write("outcome | gt | predicted\n")
        for row in jiwer_alignment:
            f.write(f"{row}\n")

def write_2_alignments_to_file(gt: str, hyp1: str, hyp2: str, file_path: str) -> None:
    result1 = get_jiwer_result(gt, hyp1)
    result2 = get_jiwer_result(gt, hyp2)
    alignment1 = get_jiwer_alignment(result1)
    alignment2 = get_jiwer_alignment(result2)

    max_len = max(len(alignment1), len(alignment2))
    alignment1 += [""] * (max_len - len(alignment1))
    alignment2 += [""] * (max_len - len(alignment2))

    with open(file_path, "w", encoding="utf-8") as f:
        f.write("outcome1 | gt | hyp1 | outcome2 | gt | hyp2\n")
        for row1, row2 in zip(alignment1, alignment2):
            o1, gt1, h1 = get_row_values(row1)
            o2, gt2, h2 = get_row_values(row2)
            f.write(f"{o1} | {gt1} | {h1} | {o2} | {gt2} | {h2}\n")

def get_2_alignments_as_string_array(gt: str, hyp1: str, hyp2: str) -> list[str]:
    result1 = get_jiwer_result(gt, hyp1)
    result2 = get_jiwer_result(gt, hyp2)
    alignment1 = get_jiwer_alignment(result1)
    alignment2 = get_jiwer_alignment(result2)

    max_len = max(len(alignment1), len(alignment2))
    alignment1 += [""] * (max_len - len(alignment1))
    alignment2 += [""] * (max_len - len(alignment2))

    lines = []
    for row1, row2 in zip(alignment1, alignment2):
        o1, gt1, h1 = get_row_values(row1)
        o2, gt2, h2 = get_row_values(row2)
        lines.append(f"{o1} | {gt1} | {h1} | {o2} | {gt2} | {h2}")
    return lines

def write_alignments_to_file(lines: list[str], file_path: str) -> None:
    with open(file_path, "w", encoding="utf-8") as f:
        f.write("outcome1 | gt | hyp1 | outcome2 | gt | hyp2\n")
        for line in lines:
            f.write(f"{line}\n")

def get_wer_by_counting(gt: str, hyp1: str, hyp2: str, lines: list[str]) -> dict:
    ref_tokens = normalize_string(gt)
    ref_len = len(ref_tokens)

    def count_ops(col_offset: int) -> dict:
        subs = dels = ins = 0
        for line in lines:
            parts = [p.strip() for p in line.split("|")]
            if len(parts) < col_offset + 1:
                continue
            outcome = parts[col_offset]
            if outcome == "SUBSTITUTION":
                subs += 1
            elif outcome == "DELETION":
                dels += 1
            elif outcome == "INSERTION":
                ins += 1
        wer = round(100 * min(1.0, (subs + dels + ins) / ref_len), 2) if ref_len else 100.0
        return {
            "wer": wer,
            "substitutions": subs,
            "deletions": dels,
            "insertions": ins
        }

    return {
        "normalized_gt": " ".join(ref_tokens),
        "reference_length": ref_len,
        "hyp1": count_ops(0),
        "hyp2": count_ops(3)
    }
===FILE_END===:nltk_jiwer_utils.py
===FILE_START===:overall_summary.py
# overall_summary.py

from __future__ import annotations
from openpyxl.worksheet.worksheet import Worksheet
from summary_calculator import SummaryCalculator
from worksheet_writer import SummaryWorksheetWriter
from constants import MONTHLY_SUMMARY_WORKSHEET_NAME, SUMMARY_COL_WIDTHS

#=== start modification for overall toxic% in G4 - overall_summary.py
def add_overall_summary_sheet(workbook, metrics_worksheet: Worksheet) -> None:
  """Compatibility wrapper for existing code that uses functional approach"""
  calculator = SummaryCalculator()
  summaries = calculator.calculate_overall_summary(metrics_worksheet)
  has_speakers = calculator.has_speaker_sections(metrics_worksheet)

  writer = SummaryWorksheetWriter(workbook, MONTHLY_SUMMARY_WORKSHEET_NAME)
  writer.create_worksheet(has_speaker_column=has_speakers)

  for row_num, summary in enumerate(summaries, start=2):
    writer.write_summary_row(summary, row_num, has_speaker_column=has_speakers)

  # Write overall toxic percentage to cell G4 (or F4)
  writer.write_overall_toxic_percent(metrics_worksheet, has_speaker_column=has_speakers)

  for col_letter, width in SUMMARY_COL_WIDTHS.items():
    writer.worksheet.column_dimensions[col_letter].width = width
#=== end modification for overall toxic% in G4
===FILE_END===:overall_summary.py
===FILE_START===:session_processor.py
# session_processor.py

from __future__ import annotations
from data_models import SessionData, SpeakerSessionData, MetricsRow, TranscriptPair
from worksheet_reader import WorksheetReader
from metrics_calculator import MetricsCalculator
from constants import ENTIRE_COL_NAME, CUSTOMER_VALUE, AGENT_VALUE, NUMBER_OF_SECTIONS

class SessionProcessor:
  def __init__(self, reader: WorksheetReader, calculator: MetricsCalculator):
    self.reader = reader
    self.calculator = calculator

  def process_session(self, session: SessionData, metadata: str) -> list[MetricsRow]:
    hallucination_data = self.reader.read_hallucination_data(session.session_id)

    metrics: list[MetricsRow] = []

    entire_metric = self.calculator.calculate_metrics_for_section(
      metadata,
      session.session_id,
      ENTIRE_COL_NAME,
      session.transcript_pairs,
      hallucination_data.counts,
      hallucination_data.texts,
      is_entire=True
    )
    metrics.append(entire_metric)

    section_metrics = self.calculator.calculate_section_metrics(
      metadata,
      session.session_id,
      session.transcript_pairs,
      hallucination_data.counts,
      hallucination_data.texts
    )
    metrics.extend(section_metrics)

    return metrics

class SpeakerSessionProcessor:
  def __init__(self, reader: WorksheetReader, calculator: MetricsCalculator):
    self.reader = reader
    self.calculator = calculator

  def process_speaker_session(
      self,
      session: SpeakerSessionData,
      metadata: str
  ) -> list[MetricsRow]:
    metrics: list[MetricsRow] = []

    for speaker_type, pairs in [
      (CUSTOMER_VALUE, session.customer_pairs),
      (AGENT_VALUE, session.agent_pairs)
    ]:
      hallucination_data = self.reader.read_hallucination_data(
        session.session_id,
        vtype=speaker_type
      )

      entire_metric = self.calculator.calculate_metrics_for_section(
        metadata,
        session.session_id,
        speaker_type,
        pairs,
        hallucination_data.counts,
        hallucination_data.texts,
        is_entire=True
      )
      metrics.append(entire_metric)

      if pairs:
        section_metrics = self._process_speaker_sections(
          metadata,
          session.session_id,
          speaker_type,
          pairs,
          hallucination_data.counts,
          hallucination_data.texts
        )
        metrics.extend(section_metrics)

    return metrics

  def _process_speaker_sections(
      self,
      metadata: str,
      session_id: str,
      speaker_type: str,
      pairs: list[TranscriptPair],
      hallucination_counts: list[int],
      hallucination_texts: list[str]
  ) -> list[MetricsRow]:
    spans = self.calculator.section_splitter.compute_spans(len(pairs), NUMBER_OF_SECTIONS)
    metrics: list[MetricsRow] = []

    for section_number, (start, end) in enumerate(spans, start=1):
      section_pairs = pairs[start:end]
      section_halluc_counts = hallucination_counts[start:end]
      section_halluc_texts = hallucination_texts[start:end]
      section_label = f"{speaker_type}{section_number}"

      metrics.append(
        self.calculator.calculate_metrics_for_section(
          metadata,
          session_id,
          section_label,
          section_pairs,
          section_halluc_counts,
          section_halluc_texts,
          is_entire=False
        )
      )

    return metrics
===FILE_END===:session_processor.py
===FILE_START===:speaker_metrics_styling.py
# speaker_metrics_styling.py
# Styling functions specific to agent-customer metrics
# Can be easily removed/modified without affecting core functionality

from __future__ import annotations
from openpyxl.worksheet.worksheet import Worksheet
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
from openpyxl.utils import get_column_letter
from constants import (
  CALL_LEVEL_BKGND_COLOR,
  SAMPLE_LEVEL_BKGND_COLOR,
  CUSTOMER_VALUE,
  AGENT_VALUE,
  WER_COL_NAME,
  HALLUCINATION_PERCENT_COL_NAME,
  METRICS_HEADER_COL_WIDTHS,
)

THIN_SIDE = Side(border_style="thin", color="D3D3D3")
THIN_BORDER = Border(top=THIN_SIDE, bottom=THIN_SIDE, left=THIN_SIDE, right=THIN_SIDE)

def apply_agent_customer_metrics_styling(worksheet: Worksheet) -> None:
  """Apply styling to agent-customer-metrics worksheet in monthly workbook"""
  style_header_row(worksheet)
  apply_freeze_and_filter(worksheet)
  set_column_widths(worksheet)
  format_data_cells(worksheet)

def style_header_row(worksheet: Worksheet) -> None:
  """Style the header row with bold font"""
  for cell in worksheet[1]:
    cell.font = Font(name="Aptos", size=9, bold=True)
    cell.alignment = Alignment("left", "center")

def apply_freeze_and_filter(worksheet: Worksheet) -> None:
  """Apply freeze panes and auto filter"""
  worksheet.freeze_panes = "A2"
  worksheet.auto_filter.ref = f"A1:{get_column_letter(worksheet.max_column)}{worksheet.max_row}"

def set_column_widths(worksheet: Worksheet) -> None:
  """Set column widths from constants"""
  for col_letter, width in METRICS_HEADER_COL_WIDTHS.items():
    worksheet.column_dimensions[col_letter].width = width

def format_data_cells(worksheet: Worksheet) -> None:
  """Format data cells with background colors, borders, and number formats"""
  header_map = {c.value: idx for idx, c in enumerate(worksheet[1], start=1)}
  wer_index = header_map.get(WER_COL_NAME, -1) - 1
  hallucination_percent_index = header_map.get(HALLUCINATION_PERCENT_COL_NAME, -1) - 1
  section_index = header_map.get("section", -1) - 1

  for row in worksheet.iter_rows(min_row=2):
    if 0 <= section_index < len(row):
      section_value = row[section_index].value
      is_speaker_level = section_value in (CUSTOMER_VALUE, AGENT_VALUE, "entire")
      background_color = CALL_LEVEL_BKGND_COLOR if is_speaker_level else SAMPLE_LEVEL_BKGND_COLOR
    else:
      background_color = SAMPLE_LEVEL_BKGND_COLOR

    for cell in row:
      cell.font = Font(name="Aptos", size=9)
      cell.fill = PatternFill("solid", fgColor=background_color)
      cell.border = THIN_BORDER

    if 0 <= wer_index < len(row) and row[wer_index].value is not None:
      row[wer_index].number_format = "0.00"

    if 0 <= hallucination_percent_index < len(row) and row[
      hallucination_percent_index].value is not None:
      row[hallucination_percent_index].number_format = "0.00"

def style_input_data_worksheet(worksheet: Worksheet) -> None:
  """Apply styling to input-data worksheet in monthly workbook"""
  worksheet.freeze_panes = "A2"
  worksheet.auto_filter.ref = f"A1:{get_column_letter(worksheet.max_column)}{worksheet.max_row}"

  for col in range(1, worksheet.max_column + 1):
    worksheet.column_dimensions[get_column_letter(col)].width = 35

  for cell in worksheet[1]:
    cell.font = Font(name="Aptos", size=9, bold=True)

  for row in worksheet.iter_rows(min_row=2):
    for cell in row:
      cell.font = Font(name="Aptos", size=9)

def style_monthly_summary_worksheet(worksheet: Worksheet, has_speaker_column: bool) -> None:
  """Apply styling to monthly-summary worksheet"""
  from constants import SUMMARY_COL_WIDTHS

  for col_letter, width in SUMMARY_COL_WIDTHS.items():
    worksheet.column_dimensions[col_letter].width = width

  for cell in worksheet[1]:
    cell.font = Font(name="Aptos", size=9, bold=True)
    cell.alignment = Alignment("left", "center")

  wer_col = 3 if has_speaker_column else 2
  hallucination_col = 5 if has_speaker_column else 4

  for row in worksheet.iter_rows(min_row=2):
    for cell in row:
      cell.border = THIN_BORDER

    if len(row) > wer_col - 1 and row[wer_col - 1].value is not None:
      row[wer_col - 1].number_format = "0.00"

    if len(row) > hallucination_col - 1 and row[hallucination_col - 1].value is not None:
      row[hallucination_col - 1].number_format = "0.00"
===FILE_END===:speaker_metrics_styling.py
===FILE_START===:styling.py
#styling.py

from __future__ import annotations
from openpyxl.worksheet.worksheet import Worksheet
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
from openpyxl.utils import get_column_letter
from constants import (
    CALL_LEVEL_BKGND_COLOR,
    SAMPLE_LEVEL_BKGND_COLOR,
    ENTIRE_COL_NAME,
    WER_COL_NAME,
    HALLUCINATION_PERCENT_COL_NAME,
    SOURCE_COL_WIDTHS,
    METRICS_HEADER_COL_WIDTHS,
)

THIN_SIDE = Side(border_style="thin", color="D3D3D3")
THIN_BORDER = Border(top=THIN_SIDE, bottom=THIN_SIDE, left=THIN_SIDE, right=THIN_SIDE)

def set_column_widths_by_letters(worksheet: Worksheet, letter_to_width: dict[str, float]) -> None:
    for letter, width in letter_to_width.items():
        worksheet.column_dimensions[letter].width = width

def style_copied_input_worksheet(worksheet: Worksheet) -> None:
    set_column_widths_by_letters(worksheet, SOURCE_COL_WIDTHS)
    worksheet.freeze_panes = "A2"
    worksheet.auto_filter.ref = f"A1:D{worksheet.max_row}"
    for cell in worksheet[1]:
        cell.font = Font(name="Aptos", size=9, bold=True)

def style_metrics_header(worksheet: Worksheet) -> None:
    for cell in worksheet[1]:
        cell.font = Font(name="Aptos", size=9, bold=True)
        cell.alignment = Alignment("left", "center")
    worksheet.freeze_panes = "A2"
    worksheet.auto_filter.ref = f"A1:{get_column_letter(worksheet.max_column)}{worksheet.max_row}"
    set_column_widths_by_letters(worksheet, METRICS_HEADER_COL_WIDTHS)

#=== start modification for reqs column - styling.py
def style_metrics_columns(worksheet: Worksheet) -> None:
    # Columns: A-metadata, B-session_id, C-section, D-gt_section, E-model_text,
    #          F-WER, G-halluc_count, H-GT_Toks, I-Model_Toks, J-halluc_%,
    #          K-halluc_text, L-Toxic, M-Toxic%, N-duration, O-Reqs, P-Reqs/Min.
    for col_index, width in {1: 27, 2: 65, 3: 9, 4: 63, 5: 63, 6: 9, 7: 12, 8: 10, 9: 10, 10: 15, 11: 12, 12: 20, 13: 10, 14: 12, 15: 10, 16: 12}.items():
        worksheet.column_dimensions[get_column_letter(col_index)].width = width
    worksheet.auto_filter.ref = f"A1:P{worksheet.max_row}"
#=== end modification for reqs column

def format_metrics_cells(worksheet: Worksheet) -> None:
    header_map = {c.value: idx for idx, c in enumerate(worksheet[1], start=1)}
    wer_index = header_map[WER_COL_NAME] - 1
    hallucination_percent_index = header_map[HALLUCINATION_PERCENT_COL_NAME] - 1
    section_index = header_map["section"] - 1
    for row in worksheet.iter_rows(min_row=2):
        if row[wer_index].value is not None:
            row[wer_index].number_format = "0.00"
        if row[hallucination_percent_index].value is not None:
            row[hallucination_percent_index].number_format = "0.00"
        background_color = CALL_LEVEL_BKGND_COLOR if row[section_index].value == ENTIRE_COL_NAME else SAMPLE_LEVEL_BKGND_COLOR
        for cell in row:
            cell.fill = PatternFill("solid", fgColor=background_color)
            cell.border = THIN_BORDER

def set_weekly_summary_column_widths(worksheet: Worksheet) -> None:
    widths = [20, 9, 12, 16]
    for col_index, width in enumerate(widths, start=1):
        worksheet.column_dimensions[get_column_letter(col_index)].width = width

===FILE_END===:styling.py
===FILE_START===:summary_calculator.py
# summary_calculator.py

from __future__ import annotations
import re
from datetime import datetime
from openpyxl.worksheet.worksheet import Worksheet
from data_models import SummaryData
#=== start modification for Both row - summary_calculator.py
from constants import (
  METADATA_COL_NAME,
  WER_COL_NAME,
  HALLUCINATION_PERCENT_COL_NAME,
  SECTION_COL_NAME,
  ENTIRE_COL_NAME,
  CUSTOMER_VALUE,
  AGENT_VALUE,
  BOTH_VALUE,
  CALL_LEVEL_GRANULARITY,
  SPREADSHEET_PREFIX,
  TRANSCRIPT_DENSITY_COL_NAME,
  MODEL_TOKS_COL_NAME,
  TOXIC_COL_NAME,
  REQS_COL_NAME,
  S_COL_NAME,
  I_COL_NAME,
  D_COL_NAME,
  GT_TOKS_COL_NAME,
  HALLUCINATION_COUNT_COL_NAME,
  DURATION_COL_NAME,
  SESSION_ID_COL_NAME,
)
#=== end modification for Both row

class MetadataNormalizer:
  @staticmethod
  def normalize(rows: list[list[str]], meta_index: int, prefix: str) -> str:
    if not rows or not rows[0]:
      return f"{prefix}_unknown"

    if '_week_of_' in rows[0][meta_index]:
      return MetadataNormalizer._normalize_week_based(rows, prefix)

    metas = [r[meta_index] for r in rows if len(r) > meta_index and r[meta_index]]
    if not metas:
      return f"{prefix}_unknown"

    parts = [re.search(r"_(\d{4})(?:thru|-)(\d{4})_(\d{4})", m) for m in metas if m]
    parts = [p for p in parts if p]

    if not parts:
      return f"{prefix}_unknown"

    earliest = min(p.group(1) for p in parts)
    latest = max(p.group(2) for p in parts)
    year = parts[0].group(3)
    return f"{prefix}_{earliest}-{latest}_{year}"

  @staticmethod
  def _normalize_week_based(rows: list[list[str]], prefix: str) -> str:
    month_day = rows[0][0][-4:]
    year = datetime.now().year
    return f"{prefix}_{month_day}_{year}"

class SummaryCalculator:
  def __init__(self):
    self.normalizer = MetadataNormalizer()

  def has_speaker_sections(self, metrics_worksheet: Worksheet) -> bool:
    header_map = {c.value: idx for idx, c in enumerate(metrics_worksheet[1], start=1)}
    section_index = header_map[SECTION_COL_NAME] - 1

    for row in metrics_worksheet.iter_rows(min_row=2, max_row=10, values_only=True):
      section_value = row[section_index]
      if section_value in (CUSTOMER_VALUE, AGENT_VALUE):
        return True
    return False

  # === start modification for timestamp - summary_calculator.py
  def calculate_overall_summary(self, metrics_worksheet: Worksheet) -> list[SummaryData]:
    self.metrics_worksheet = metrics_worksheet
    header_map = {c.value: idx for idx, c in enumerate(metrics_worksheet[1], start=1)}
    rows = list(metrics_worksheet.iter_rows(min_row=2, values_only=True))

    wer_index = header_map[WER_COL_NAME] - 1
    hallucination_percent_index = header_map[HALLUCINATION_PERCENT_COL_NAME] - 1
    section_index = header_map[SECTION_COL_NAME] - 1
    metadata_index = header_map[METADATA_COL_NAME] - 1

    has_speakers = self.has_speaker_sections(metrics_worksheet)

    if has_speakers:
      return self._calculate_speaker_summary(
        rows, wer_index, hallucination_percent_index, section_index, metadata_index
      )
    else:
      return self._calculate_non_speaker_summary(
        rows, wer_index, hallucination_percent_index, section_index, metadata_index
      )
  # === end modification for timestamp

  # === start modification for reqs column - summary_calculator.py
  def _calculate_speaker_summary(
      self,
      rows: list,
      wer_index: int,
      hallucination_percent_index: int,
      section_index: int,
      metadata_index: int
  ) -> list[SummaryData]:
    summaries: list[SummaryData] = []

    density_index = None
    model_toks_index = None
    toxic_index = None
    reqs_index = None

    for idx, cell in enumerate(self.metrics_worksheet[1]):
      if cell.value == TRANSCRIPT_DENSITY_COL_NAME:
        density_index = idx
      elif cell.value == MODEL_TOKS_COL_NAME:
        model_toks_index = idx
      elif cell.value == TOXIC_COL_NAME:
        toxic_index = idx
      elif cell.value == REQS_COL_NAME:
        reqs_index = idx

    for speaker in [CUSTOMER_VALUE, AGENT_VALUE]:
      if CALL_LEVEL_GRANULARITY:
        wer_values = [
          row[wer_index] for row in rows
          if row[section_index] == speaker and isinstance(row[wer_index], (int, float))
        ]
        hallucination_values = [
          row[hallucination_percent_index] for row in rows
          if row[section_index] == speaker and isinstance(row[hallucination_percent_index], (int, float))
        ]
        density_values = [
          row[density_index] for row in rows
          if density_index is not None and row[section_index] == speaker
             and isinstance(row[density_index], (int, float))
        ] if density_index is not None else []

        # Get reqs value for this speaker
        reqs = 0
        if reqs_index is not None:
          for row in rows:
            if row[section_index] == speaker:
              reqs_value = row[reqs_index]
              if isinstance(reqs_value, (int, float)):
                reqs = int(reqs_value)
                break  # Use the first matching row's reqs value

        # Calculate toxic% for this speaker
        total_model_tokens = 0
        total_toxic_count = 0
        if model_toks_index is not None and toxic_index is not None:
          for row in rows:
            if row[section_index] == speaker:
              model_toks = row[model_toks_index]
              toxic_text = row[toxic_index]

              if isinstance(model_toks, (int, float)):
                total_model_tokens += model_toks

              if toxic_text and isinstance(toxic_text, str):
                # Count toxic tokens (comma-separated)
                toxic_tokens = [t.strip() for t in toxic_text.split(',') if t.strip()]
                total_toxic_count += len(toxic_tokens)
      else:
        wer_values = [
          row[wer_index] for row in rows
          if isinstance(row[section_index], str) and row[section_index].startswith(speaker)
             and row[section_index] != speaker and isinstance(row[wer_index], (int, float))
        ]
        hallucination_values = [
          row[hallucination_percent_index] for row in rows
          if isinstance(row[section_index], str) and row[section_index].startswith(speaker)
             and row[section_index] != speaker and isinstance(row[hallucination_percent_index], (int, float))
        ]
        density_values = [
          row[density_index] for row in rows
          if density_index is not None
             and isinstance(row[section_index], str)
             and row[section_index].startswith(speaker)
             and row[section_index] != speaker
             and isinstance(row[density_index], (int, float))
        ] if density_index is not None else []

        # Get reqs value for this speaker (sum all section reqs for sample level)
        reqs = 0
        if reqs_index is not None:
          for row in rows:
            if (isinstance(row[section_index], str) and
                row[section_index].startswith(speaker) and
                row[section_index] != speaker):
              reqs_value = row[reqs_index]
              if isinstance(reqs_value, (int, float)):
                reqs += int(reqs_value)

        # Calculate toxic% for this speaker (sample level)
        total_model_tokens = 0
        total_toxic_count = 0
        if model_toks_index is not None and toxic_index is not None:
          for row in rows:
            if (isinstance(row[section_index], str) and
                row[section_index].startswith(speaker) and
                row[section_index] != speaker):
              model_toks = row[model_toks_index]
              toxic_text = row[toxic_index]

              if isinstance(model_toks, (int, float)):
                total_model_tokens += model_toks

              if toxic_text and isinstance(toxic_text, str):
                # Count toxic tokens (comma-separated)
                toxic_tokens = [t.strip() for t in toxic_text.split(',') if t.strip()]
                total_toxic_count += len(toxic_tokens)

      average_wer = sum(wer_values) / len(wer_values) if wer_values else 0.0
      average_hallucination = sum(hallucination_values) / len(hallucination_values) if hallucination_values else 0.0
      average_density = sum(density_values) / len(density_values) if density_values else None
      toxic_percent = round((100 * total_toxic_count / total_model_tokens), 2) if total_model_tokens > 0 else None
      session_count = int(sum(1 for row in rows if row[section_index] == speaker))
      normalized_meta = self.normalizer.normalize(rows, metadata_index, SPREADSHEET_PREFIX)

      summaries.append(SummaryData(
        metadata=normalized_meta,
        average_wer=average_wer,
        session_count=session_count,
        average_hallucination_percent=average_hallucination,
        reqs=reqs,
        speaker=speaker,
        average_transcript_density=average_density,
        toxic_percent=toxic_percent
      ))

    # === start modification for Both row - summary_calculator.py
    # Calculate "Both" row combining Customer and Agent metrics
    # Get additional column indices needed for "Both" calculation
    s_index = i_index = d_index = gt_toks_index = halluc_count_index = duration_index = session_id_index = None
    for idx, cell in enumerate(self.metrics_worksheet[1]):
      if cell.value == S_COL_NAME:
        s_index = idx
      elif cell.value == I_COL_NAME:
        i_index = idx
      elif cell.value == D_COL_NAME:
        d_index = idx
      elif cell.value == GT_TOKS_COL_NAME:
        gt_toks_index = idx
      elif cell.value == HALLUCINATION_COUNT_COL_NAME:
        halluc_count_index = idx
      elif cell.value == DURATION_COL_NAME:
        duration_index = idx
      elif cell.value == SESSION_ID_COL_NAME:
        session_id_index = idx

    if session_id_index is not None:
      # Group rows by session_id, keeping only Customer and Agent rows
      sessions = {}
      for row in rows:
        if row[section_index] in (CUSTOMER_VALUE, AGENT_VALUE):
          sid = row[session_id_index]
          if sid not in sessions:
            sessions[sid] = {}
          sessions[sid][row[section_index]] = row

      # Calculate per-session combined metrics
      combined_wer_values = []
      combined_halluc_values = []
      combined_reqs_min_values = []
      total_model_tokens_both = 0
      total_toxic_count_both = 0

      for sid, speaker_rows in sessions.items():
        if CUSTOMER_VALUE in speaker_rows and AGENT_VALUE in speaker_rows:
          cust_row = speaker_rows[CUSTOMER_VALUE]
          agent_row = speaker_rows[AGENT_VALUE]

          # Combined WER: (S+I+D) / GT_Toks for both speakers
          if s_index is not None and i_index is not None and d_index is not None and gt_toks_index is not None:
            cust_s = cust_row[s_index] if isinstance(cust_row[s_index], (int, float)) else 0
            cust_i = cust_row[i_index] if isinstance(cust_row[i_index], (int, float)) else 0
            cust_d = cust_row[d_index] if isinstance(cust_row[d_index], (int, float)) else 0
            agent_s = agent_row[s_index] if isinstance(agent_row[s_index], (int, float)) else 0
            agent_i = agent_row[i_index] if isinstance(agent_row[i_index], (int, float)) else 0
            agent_d = agent_row[d_index] if isinstance(agent_row[d_index], (int, float)) else 0
            cust_gt = cust_row[gt_toks_index] if isinstance(cust_row[gt_toks_index], (int, float)) else 0
            agent_gt = agent_row[gt_toks_index] if isinstance(agent_row[gt_toks_index], (int, float)) else 0

            total_errors = cust_s + cust_i + cust_d + agent_s + agent_i + agent_d
            total_gt = cust_gt + agent_gt
            if total_gt > 0:
              combined_wer_values.append(100 * total_errors / total_gt)

          # Combined Hallucination%: 100 * (halluc_count) / (model_toks)
          if halluc_count_index is not None and model_toks_index is not None:
            cust_halluc = cust_row[halluc_count_index] if isinstance(cust_row[halluc_count_index], (int, float)) else 0
            agent_halluc = agent_row[halluc_count_index] if isinstance(agent_row[halluc_count_index], (int, float)) else 0
            cust_model = cust_row[model_toks_index] if isinstance(cust_row[model_toks_index], (int, float)) else 0
            agent_model = agent_row[model_toks_index] if isinstance(agent_row[model_toks_index], (int, float)) else 0

            total_halluc = cust_halluc + agent_halluc
            total_model = cust_model + agent_model
            if total_model > 0:
              combined_halluc_values.append(100 * total_halluc / total_model)

            # Accumulate for toxic% (sum-then-divide)
            total_model_tokens_both += total_model

          # Combined Reqs/Min: (reqs) / (duration in minutes)
          if reqs_index is not None and duration_index is not None:
            cust_reqs = cust_row[reqs_index] if isinstance(cust_row[reqs_index], (int, float)) else 0
            agent_reqs = agent_row[reqs_index] if isinstance(agent_row[reqs_index], (int, float)) else 0
            cust_dur = cust_row[duration_index] if isinstance(cust_row[duration_index], (int, float)) else 0
            agent_dur = agent_row[duration_index] if isinstance(agent_row[duration_index], (int, float)) else 0

            total_reqs = cust_reqs + agent_reqs
            total_dur = cust_dur + agent_dur  # Duration is already in minutes
            if total_dur > 0:
              combined_reqs_min_values.append(total_reqs / total_dur)

          # Accumulate toxic counts for both speakers
          if toxic_index is not None:
            for speaker_row in [cust_row, agent_row]:
              toxic_text = speaker_row[toxic_index]
              if toxic_text and isinstance(toxic_text, str):
                toxic_tokens = [t.strip() for t in toxic_text.split(',') if t.strip()]
                total_toxic_count_both += len(toxic_tokens)

      # Calculate averages for "Both" row
      both_avg_wer = sum(combined_wer_values) / len(combined_wer_values) if combined_wer_values else 0.0
      both_avg_halluc = sum(combined_halluc_values) / len(combined_halluc_values) if combined_halluc_values else 0.0
      both_avg_reqs_min = sum(combined_reqs_min_values) / len(combined_reqs_min_values) if combined_reqs_min_values else None
      both_toxic_percent = round((100 * total_toxic_count_both / total_model_tokens_both), 2) if total_model_tokens_both > 0 else None

      # Use same metadata and session_count as previous rows
      both_metadata = summaries[0].metadata if summaries else normalized_meta
      both_session_count = summaries[0].session_count if summaries else session_count

      summaries.append(SummaryData(
        metadata=both_metadata,
        average_wer=both_avg_wer,
        session_count=both_session_count,
        average_hallucination_percent=both_avg_halluc,
        reqs=0,  # Not used for Both row
        speaker=BOTH_VALUE,
        average_transcript_density=both_avg_reqs_min,  # Using this field for Avg. Reqs/Min.
        toxic_percent=both_toxic_percent
      ))
    # === end modification for Both row

    return summaries
  # === end modification for reqs column

  # === start modification for reqs column - summary_calculator.py
  def _calculate_non_speaker_summary(
      self,
      rows: list,
      wer_index: int,
      hallucination_percent_index: int,
      section_index: int,
      metadata_index: int
  ) -> list[SummaryData]:
    density_index = None
    model_toks_index = None
    toxic_index = None
    reqs_index = None

    for idx, cell in enumerate(self.metrics_worksheet[1]):
      if cell.value == TRANSCRIPT_DENSITY_COL_NAME:
        density_index = idx
      elif cell.value == MODEL_TOKS_COL_NAME:
        model_toks_index = idx
      elif cell.value == TOXIC_COL_NAME:
        toxic_index = idx
      elif cell.value == REQS_COL_NAME:
        reqs_index = idx

    if CALL_LEVEL_GRANULARITY:
      wer_values = [
        row[wer_index] for row in rows
        if row[section_index] == ENTIRE_COL_NAME and isinstance(row[wer_index], (int, float))
      ]
      hallucination_values = [
        row[hallucination_percent_index] for row in rows
        if row[section_index] == ENTIRE_COL_NAME and isinstance(row[hallucination_percent_index], (int, float))
      ]
      density_values = [
        row[density_index] for row in rows
        if density_index is not None and row[section_index] == ENTIRE_COL_NAME
           and isinstance(row[density_index], (int, float))
      ] if density_index is not None else []

      # Get reqs value
      reqs = 0
      if reqs_index is not None:
        for row in rows:
          if row[section_index] == ENTIRE_COL_NAME:
            reqs_value = row[reqs_index]
            if isinstance(reqs_value, (int, float)):
              reqs = int(reqs_value)
              break

      # Calculate toxic% (call level)
      total_model_tokens = 0
      total_toxic_count = 0
      if model_toks_index is not None and toxic_index is not None:
        for row in rows:
          if row[section_index] == ENTIRE_COL_NAME:
            model_toks = row[model_toks_index]
            toxic_text = row[toxic_index]

            if isinstance(model_toks, (int, float)):
              total_model_tokens += model_toks

            if toxic_text and isinstance(toxic_text, str):
              # Count toxic tokens (comma-separated)
              toxic_tokens = [t.strip() for t in toxic_text.split(',') if t.strip()]
              total_toxic_count += len(toxic_tokens)
    else:
      wer_values = [
        row[wer_index] for row in rows
        if row[section_index] != ENTIRE_COL_NAME and isinstance(row[wer_index], (int, float))
      ]
      hallucination_values = [
        row[hallucination_percent_index] for row in rows
        if row[section_index] != ENTIRE_COL_NAME and isinstance(row[hallucination_percent_index], (int, float))
      ]
      density_values = [
        row[density_index] for row in rows
        if density_index is not None and row[section_index] != ENTIRE_COL_NAME
           and isinstance(row[density_index], (int, float))
      ] if density_index is not None else []

      # Get reqs value (sum all sections for sample level)
      reqs = 0
      if reqs_index is not None:
        for row in rows:
          if row[section_index] != ENTIRE_COL_NAME:
            reqs_value = row[reqs_index]
            if isinstance(reqs_value, (int, float)):
              reqs += int(reqs_value)

      # Calculate toxic% (sample level)
      total_model_tokens = 0
      total_toxic_count = 0
      if model_toks_index is not None and toxic_index is not None:
        for row in rows:
          if row[section_index] != ENTIRE_COL_NAME:
            model_toks = row[model_toks_index]
            toxic_text = row[toxic_index]

            if isinstance(model_toks, (int, float)):
              total_model_tokens += model_toks

            if toxic_text and isinstance(toxic_text, str):
              # Count toxic tokens (comma-separated)
              toxic_tokens = [t.strip() for t in toxic_text.split(',') if t.strip()]
              total_toxic_count += len(toxic_tokens)

    average_wer = sum(wer_values) / len(wer_values) if wer_values else 0.0
    average_hallucination = sum(hallucination_values) / len(hallucination_values) if hallucination_values else 0.0
    average_density = sum(density_values) / len(density_values) if density_values else None
    toxic_percent = round((100 * total_toxic_count / total_model_tokens), 2) if total_model_tokens > 0 else None
    normalized_meta = self.normalizer.normalize(rows, metadata_index, SPREADSHEET_PREFIX)
    session_count = int(sum(1 for row in rows if row[section_index] == ENTIRE_COL_NAME))

    return [SummaryData(
      metadata=normalized_meta,
      average_wer=average_wer,
      session_count=session_count,
      average_hallucination_percent=average_hallucination,
      reqs=reqs,
      average_transcript_density=average_density,
      toxic_percent=toxic_percent
    )]
  # === end modification for reqs column
===FILE_END===:summary_calculator.py
===FILE_START===:weekly_summary.py
# weekly_summary.py

from __future__ import annotations
from collections import defaultdict
from openpyxl.worksheet.worksheet import Worksheet
from openpyxl.styles import Font, PatternFill
from constants import (
  METADATA_COL_NAME,
  AVERAGE_WER_COL_NAME,
  SESSION_COUNT_COL_NAME,
  AVG_HALLUCINATION_PERCENT_COL_NAME,
  ENTIRE_COL_NAME,
  WER_COL_NAME,
  HALLUCINATION_PERCENT_COL_NAME,
  WEEKLY_SUMMARY_WORKSHEET_NAME,
  AVERAGING_PERIOD,
  SPREADSHEET_PREFIX,
  CALL_LEVEL_BKGND_COLOR,
)
from styling import set_weekly_summary_column_widths

def add_weekly_summary_sheet(workbook) -> Worksheet:
  if WEEKLY_SUMMARY_WORKSHEET_NAME in workbook.sheetnames:
    del workbook[WEEKLY_SUMMARY_WORKSHEET_NAME]
  return workbook.create_sheet(WEEKLY_SUMMARY_WORKSHEET_NAME)

def write_weekly_summary(metrics_worksheet: Worksheet, summary_worksheet: Worksheet) -> None:
  write_weekly_summary_header(summary_worksheet)
  weekly_summaries = extract_metadata_groups(metrics_worksheet)
  write_weekly_summary_rows(summary_worksheet, weekly_summaries, start_row=3)
  header_map = {c.value: idx for idx, c in enumerate(summary_worksheet[1], start=1)}
  meta_index = header_map[METADATA_COL_NAME] - 1
  avg_wer_index = header_map[AVERAGE_WER_COL_NAME] - 1
  avg_hallucination_index = header_map[AVG_HALLUCINATION_PERCENT_COL_NAME] - 1
  session_index = header_map[SESSION_COUNT_COL_NAME] - 1
  summaries_from_sheet: dict[str, list[tuple[float, float, float]]] = {}
  for row in summary_worksheet.iter_rows(min_row=3, values_only=True):
    meta_label = row[meta_index]
    if not meta_label:
      continue
    w = row[avg_wer_index]
    h = row[avg_hallucination_index]
    s = row[session_index]
    summaries_from_sheet.setdefault(meta_label, []).append((w, s, h))
  write_weekly_summary_final_row(summary_worksheet, summaries_from_sheet)
  set_weekly_summary_column_widths(summary_worksheet)

def write_weekly_summary_header(summary_worksheet: Worksheet) -> None:
  titles = [METADATA_COL_NAME, AVERAGE_WER_COL_NAME, SESSION_COUNT_COL_NAME, AVG_HALLUCINATION_PERCENT_COL_NAME]
  for index, title in enumerate(titles, start=1):
    cell = summary_worksheet.cell(1, index, title)
    cell.font = Font(name="Aptos", size=9, bold=True)

def extract_metadata_groups(metrics_worksheet: Worksheet) -> dict[str, list[tuple[float, float]]]:
  header_map = {c.value: idx for idx, c in enumerate(metrics_worksheet[1], start=1)}
  metadata_index = header_map[METADATA_COL_NAME] - 1
  wer_index = header_map[WER_COL_NAME] - 1
  hallucination_percent_index = header_map[HALLUCINATION_PERCENT_COL_NAME] - 1
  section_index = header_map["section"] - 1
  groups = defaultdict(list)
  for row in metrics_worksheet.iter_rows(min_row=2, values_only=True):
    if row[section_index] == ENTIRE_COL_NAME:
      metadata_value = str(row[metadata_index])
      wer_value = row[wer_index]
      hallucination_value = row[hallucination_percent_index]
      if wer_value is not None:
        groups[metadata_value].append((wer_value, hallucination_value))
  return groups

def write_weekly_summary_rows(summary_worksheet: Worksheet, summaries: dict[str, list[tuple[float, float]]],
                              start_row: int) -> None:
  for row_offset, (metadata_value, rows) in enumerate(summaries.items(), start=start_row):
    wer_values = [w for w, _ in rows]
    hallucination_values = [h for _, h in rows if h is not None]
    average_wer = round(sum(wer_values) / len(wer_values), 4) if wer_values else None
    average_hallucination = round(sum(hallucination_values) / len(hallucination_values),
                                  5) if hallucination_values else None
    count = len(wer_values)
    summary_worksheet.cell(row_offset, 1, metadata_value)
    summary_worksheet.cell(row_offset, 2, average_wer)
    summary_worksheet.cell(row_offset, 3, count)
    summary_worksheet.cell(row_offset, 4, average_hallucination)
    for col in range(1, 5):
      summary_worksheet.cell(row_offset, col).font = Font(name="Aptos", size=9)

def write_weekly_summary_final_row(summary_worksheet: Worksheet,
                                   summaries: dict[str, list[tuple[float, float, float]]]) -> None:
  all_rows = []
  for metadata_value, rows in summaries.items():
    for wer, sessions, hallucinations in rows:
      all_rows.append((metadata_value, wer, sessions, hallucinations))

  wer_values = [row[1] for row in all_rows if isinstance(row[1], (int, float))]
  average_wer = (sum(wer_values) / len(wer_values)) if wer_values else None
  session_values = [row[2] for row in all_rows if isinstance(row[2], (int, float))]
  total_sessions = int(sum(session_values)) if session_values else 0
  hallucination_values = [row[3] for row in all_rows if isinstance(row[3], (int, float))]
  average_hallucination = (sum(hallucination_values) / len(hallucination_values)) if hallucination_values else None

  metadata_rows = [[row[0] for row in all_rows]]

  # Import the normalizer from the OOP class
  from summary_calculator import MetadataNormalizer
  normalizer = MetadataNormalizer()
  normalized_metadata = normalizer.normalize(metadata_rows, 0, SPREADSHEET_PREFIX)

  final_row_index = len(summaries) + 4
  summary_worksheet.cell(final_row_index, 1, normalized_metadata)
  c2 = summary_worksheet.cell(final_row_index, 2, average_wer)
  c3 = summary_worksheet.cell(final_row_index, 3, total_sessions)
  c4 = summary_worksheet.cell(final_row_index, 4, average_hallucination)

  for col in (1, 2, 3, 4):
    cell = summary_worksheet.cell(final_row_index, col)
    cell.font = Font(name="Aptos", size=9)
    cell.fill = PatternFill("solid", fgColor=CALL_LEVEL_BKGND_COLOR)

  c2.number_format = "0.000"
  c4.number_format = "0.000"
===FILE_END===:weekly_summary.py
===FILE_START===:workbook_processor.py
# workbook_processor.py

from __future__ import annotations
import os
from typing import Optional
from openpyxl import load_workbook, Workbook
from worksheet_reader import WorksheetReader
from metrics_calculator import MetricsCalculator
from session_processor import SessionProcessor, SpeakerSessionProcessor
from worksheet_writer import MetricsWorksheetWriter
from constants import WORKSHEET_PREFIX, METRICS_WORKSHEET_NAME
from styling import style_copied_input_worksheet

class WorkbookProcessor:
  def __init__(self):
    self.metrics_calculator = MetricsCalculator()

  def process_input_workbook(self, input_path: str) -> Optional[str]:
    worksheet_name = self._find_worksheet_name(input_path)
    if not worksheet_name:
      return None

    input_workbook = load_workbook(input_path, read_only=True, data_only=True, keep_links=False)

    try:
      input_worksheet = input_workbook[worksheet_name]
      reader = WorksheetReader(input_worksheet)

      output_path = self._get_output_path(input_path)
      if os.path.exists(output_path):
        os.remove(output_path)

      output_workbook = Workbook()
      self._copy_input_worksheet(output_workbook, input_worksheet, worksheet_name)

      metadata = os.path.basename(input_path).rsplit(".", 1)[0]
      self._generate_metrics(output_workbook, reader, metadata)

      from weekly_summary import add_weekly_summary_sheet, write_weekly_summary
      from overall_summary import add_overall_summary_sheet

      metrics_ws = output_workbook[METRICS_WORKSHEET_NAME]
      weekly_ws = add_weekly_summary_sheet(output_workbook)
      write_weekly_summary(metrics_ws, weekly_ws)
      add_overall_summary_sheet(output_workbook, metrics_ws)

      output_workbook.save(output_path)
      return output_path

    finally:
      input_workbook.close()

  def _find_worksheet_name(self, input_path: str) -> Optional[str]:
    try:
      workbook = load_workbook(input_path, read_only=True, data_only=True, keep_links=False)
      try:
        return next((n for n in workbook.sheetnames if n.startswith(WORKSHEET_PREFIX)), None)
      finally:
        workbook.close()
    except Exception:
      return None

  def _get_output_path(self, input_path: str) -> str:
    directory = os.path.dirname(input_path)
    base = os.path.basename(input_path).rsplit(".", 1)[0]
    return os.path.join(directory, f"{base}_metrics.xlsx")

  def _copy_input_worksheet(self, output_workbook: Workbook, input_worksheet, worksheet_name: str) -> None:
    copied_worksheet = output_workbook.active
    copied_worksheet.title = worksheet_name

    for row_values in input_worksheet.iter_rows(values_only=True):
      copied_worksheet.append(row_values)

    style_copied_input_worksheet(copied_worksheet)

  def _generate_metrics(self, workbook: Workbook, reader: WorksheetReader, metadata: str) -> None:
    writer = MetricsWorksheetWriter(workbook)
    writer.create_worksheet()

    if reader.has_vtype_column():
      self._process_speaker_sessions(reader, writer, metadata)
    else:
      self._process_regular_sessions(reader, writer, metadata)

    writer.apply_styling()

  def _process_regular_sessions(self, reader: WorksheetReader, writer: MetricsWorksheetWriter, metadata: str) -> None:
    sessions = reader.read_sessions()
    processor = SessionProcessor(reader, self.metrics_calculator)

    for session in sessions.values():
      metrics_list = processor.process_session(session, metadata)
      for metrics in metrics_list:
        writer.write_metrics_row(metrics)

  def _process_speaker_sessions(self, reader: WorksheetReader, writer: MetricsWorksheetWriter, metadata: str) -> None:
    sessions = reader.read_speaker_sessions()
    processor = SpeakerSessionProcessor(reader, self.metrics_calculator)

    for session in sessions.values():
      metrics_list = processor.process_speaker_session(session, metadata)
      for metrics in metrics_list:
        writer.write_metrics_row(metrics)
===FILE_END===:workbook_processor.py
===FILE_START===:worksheet_reader.py
# worksheet_reader.py

from __future__ import annotations
from typing import Optional
from openpyxl.worksheet.worksheet import Worksheet
from data_models import TranscriptPair, SessionData, SpeakerSessionData, HallucinationData
from constants import (
  SESSION_ID_COL_NAME,
  RAW_TRANSCRIPT_COL_NAME,
  GROUND_TRUTH_COL_NAME,
  HALLUCINATION_COL_NAME,
  VTYPE_COL_NAME,
  CUSTOMER_VALUE,
  AGENT_VALUE,
  USE_HALLUCINATION_FLAGS,
  TIMESTAMP_COL_NAME,
)

class WorksheetReader:
  def __init__(self, worksheet: Worksheet):
    self.worksheet = worksheet
    self.header_map = self._build_header_map()

  def _build_header_map(self) -> dict[str, int]:
    return {cell.value: idx for idx, cell in enumerate(self.worksheet[1], start=1)}

  def has_column(self, column_name: str) -> bool:
    return column_name in self.header_map

  def has_vtype_column(self) -> bool:
    return self.has_column(VTYPE_COL_NAME)

  # === start modification for timestamp - worksheet_reader.py
  def read_sessions(self) -> dict[str, SessionData]:
    session_id_index = self.header_map[SESSION_ID_COL_NAME] - 1
    raw_transcript_index = self.header_map[RAW_TRANSCRIPT_COL_NAME] - 1
    ground_truth_index = self.header_map[GROUND_TRUTH_COL_NAME] - 1
    timestamp_index = self.header_map.get(TIMESTAMP_COL_NAME, 0) - 1 if TIMESTAMP_COL_NAME in self.header_map else None

    sessions: dict[str, list[TranscriptPair]] = {}
    prev_session_id = None
    prev_timestamp = None

    for row in self.worksheet.iter_rows(min_row=2, values_only=True):
      session_id = str(row[session_id_index] or "").strip()
      if not session_id:
        continue

      timestamp = None
      duration = 0
      if timestamp_index is not None and timestamp_index >= 0 and len(row) > timestamp_index:
        timestamp_value = row[timestamp_index]
        if timestamp_value is not None:
          timestamp = int(timestamp_value)
          if prev_session_id == session_id and prev_timestamp is not None:
            duration = timestamp - prev_timestamp

      pair = TranscriptPair(
        transcript=row[raw_transcript_index],
        ground_truth=row[ground_truth_index],
        timestamp=timestamp,
        duration=duration
      )
      sessions.setdefault(session_id, []).append(pair)

      prev_session_id = session_id
      prev_timestamp = timestamp

    return {
      sid: SessionData(session_id=sid, transcript_pairs=pairs)
      for sid, pairs in sessions.items()
    }
  # === end modification for timestamp

  # === start modification for timestamp - worksheet_reader.py
  def read_speaker_sessions(self) -> dict[str, SpeakerSessionData]:
    session_id_index = self.header_map[SESSION_ID_COL_NAME] - 1
    raw_transcript_index = self.header_map[RAW_TRANSCRIPT_COL_NAME] - 1
    ground_truth_index = self.header_map[GROUND_TRUTH_COL_NAME] - 1
    vtype_index = self.header_map[VTYPE_COL_NAME] - 1
    timestamp_index = self.header_map.get(TIMESTAMP_COL_NAME, 0) - 1 if TIMESTAMP_COL_NAME in self.header_map else None

    sessions: dict[str, SpeakerSessionData] = {}
    prev_session_id = None
    prev_timestamp = None

    for row in self.worksheet.iter_rows(min_row=2, values_only=True):
      session_id = str(row[session_id_index] or "").strip()
      if not session_id:
        continue

      if session_id not in sessions:
        sessions[session_id] = SpeakerSessionData(session_id=session_id)

      timestamp = None
      duration = 0
      if timestamp_index is not None and timestamp_index >= 0 and len(row) > timestamp_index:
        timestamp_value = row[timestamp_index]
        if timestamp_value is not None:
          timestamp = int(timestamp_value)
          if prev_session_id == session_id and prev_timestamp is not None:
            duration = timestamp - prev_timestamp

      vtype = str(row[vtype_index] or "").strip()
      pair = TranscriptPair(
        transcript=row[raw_transcript_index],
        ground_truth=row[ground_truth_index],
        timestamp=timestamp,
        duration=duration
      )

      if vtype == CUSTOMER_VALUE:
        sessions[session_id].customer_pairs.append(pair)
      elif vtype == AGENT_VALUE:
        sessions[session_id].agent_pairs.append(pair)

      prev_session_id = session_id
      prev_timestamp = timestamp

    return sessions
  # === end modification for timestamp

  def read_hallucination_data(self, session_id: str, vtype: Optional[str] = None) -> HallucinationData:
    session_id_index = self.header_map[SESSION_ID_COL_NAME] - 1
    hallucination_index = self.header_map[HALLUCINATION_COL_NAME] - 1
    vtype_index = self.header_map.get(VTYPE_COL_NAME, 0) - 1 if vtype else None

    counts: list[int] = []
    texts: list[str] = []

    for row in self.worksheet.iter_rows(min_row=2, values_only=True):
      if len(row) <= max(session_id_index, hallucination_index):
        continue

      row_session_id = str(row[session_id_index] or "").strip()
      if row_session_id != session_id:
        continue

      if vtype and vtype_index is not None:
        if len(row) <= vtype_index:
          continue
        row_vtype = str(row[vtype_index] or "").strip()
        if row_vtype != vtype:
          continue

      hallucination = row[hallucination_index]
      hallucination_text = str(hallucination).strip() if hallucination else ""
      texts.append(hallucination_text)

      if not hallucination:
        counts.append(0)
      elif USE_HALLUCINATION_FLAGS:
        counts.append(1)
      else:
        counts.append(len(hallucination_text.split()))

    return HallucinationData(counts=counts, texts=texts)
===FILE_END===:worksheet_reader.py
===FILE_START===:worksheet_writer.py
# worksheet_writer.py

from __future__ import annotations
from typing import Optional
from openpyxl import Workbook
from openpyxl.worksheet.worksheet import Worksheet
from openpyxl.styles import Font, Alignment, PatternFill
from data_models import MetricsRow, SummaryData
from styling import THIN_BORDER

#=== start modification for Both row - worksheet_writer.py (imports)
from constants import (
  METRICS_WORKSHEET_NAME,
  METADATA_COL_NAME,
  SESSION_ID_COL_NAME,
  SECTION_COL_NAME,
  GT_SECTION_TEXT_COL_NAME,
  MODEL_SECTION_TEXT_COL_NAME,
  WER_COL_NAME,
  HALLUCINATION_COUNT_COL_NAME,
  GT_TOKS_COL_NAME,
  MODEL_TOKS_COL_NAME,
  HALLUCINATION_PERCENT_COL_NAME,
  HALLUCINATION_COL_NAME,
  TOXIC_COL_NAME,
  TOXIC_PERCENT_COL_NAME,
  S_COL_NAME,
  I_COL_NAME,
  D_COL_NAME,
  CALL_LEVEL_BKGND_COLOR,
  SAMPLE_LEVEL_BKGND_COLOR,
  AVERAGE_WER_COL_NAME,
  SESSION_COUNT_COL_NAME,
  AVG_HALLUCINATION_PERCENT_COL_NAME,
  AVG_TOXIC_PERCENT_COL_NAME,
  CALL_LEVEL_GRANULARITY,
  DURATION_COL_NAME,
  REQS_COL_NAME,
  TRANSCRIPT_DENSITY_COL_NAME,
  AVERAGE_TRX_DENSITY_COL_NAME,
  BOTH_VALUE,
)
#=== end modification for Both row

class MetricsWorksheetWriter:
  def __init__(self, workbook: Workbook):
    self.workbook = workbook
    self.worksheet: Optional[Worksheet] = None

  def create_worksheet(self) -> Worksheet:
    if METRICS_WORKSHEET_NAME in self.workbook.sheetnames:
      del self.workbook[METRICS_WORKSHEET_NAME]
    self.worksheet = self.workbook.create_sheet(METRICS_WORKSHEET_NAME)
    self._write_header()
    return self.worksheet

  def _write_header(self) -> None:
    headers = [
      METADATA_COL_NAME,
      SESSION_ID_COL_NAME,
      SECTION_COL_NAME,
      GT_SECTION_TEXT_COL_NAME,
      MODEL_SECTION_TEXT_COL_NAME,
      WER_COL_NAME,
      S_COL_NAME,
      I_COL_NAME,
      D_COL_NAME,
      GT_TOKS_COL_NAME,
      MODEL_TOKS_COL_NAME,
      HALLUCINATION_COUNT_COL_NAME,
      HALLUCINATION_PERCENT_COL_NAME,
      HALLUCINATION_COL_NAME,
      TOXIC_COL_NAME,
      TOXIC_PERCENT_COL_NAME,
      DURATION_COL_NAME,
      REQS_COL_NAME,
      TRANSCRIPT_DENSITY_COL_NAME,
    ]

    for col_index, title in enumerate(headers, start=1):
      cell = self.worksheet.cell(1, col_index, title)
      cell.font = Font(name="Aptos", size=9, bold=True)
      cell.alignment = Alignment("left", "center")

    self.worksheet.freeze_panes = "A2"

  # === start modification for S, I, D columns - worksheet_writer.py (MetricsWorksheetWriter.write_metrics_row)
  def write_metrics_row(self, metrics: MetricsRow) -> None:
    row_index = self.worksheet.max_row + 1
    font = Font(name="Aptos", size=9)
    alignment = Alignment("left", "center")
    fill_color = CALL_LEVEL_BKGND_COLOR if metrics.is_entire else SAMPLE_LEVEL_BKGND_COLOR
    fill = PatternFill("solid", fgColor=fill_color)

    # Convert duration from seconds to minutes for display and round to 3 decimals
    duration_in_minutes = round(metrics.duration / (60 * 1), 2) if metrics.duration else 0.0

    # Round transcript_density to 2 decimal places
    transcript_density_rounded = round(metrics.transcript_density,
                                       2) if metrics.transcript_density is not None else None

    values = [
      metrics.metadata,
      metrics.session_id,
      metrics.section,
      metrics.ground_truth_text,
      metrics.model_text,
      metrics.wer,
      metrics.substitutions,
      metrics.insertions,
      metrics.deletions,
      metrics.ground_truth_tokens,
      metrics.model_tokens,
      metrics.hallucination_count,
      metrics.hallucination_percent,
      metrics.hallucination_text,
      metrics.toxic_text,
      metrics.toxic_percent,
      duration_in_minutes,
      metrics.row_count,
      transcript_density_rounded,
    ]

    for col_index, value in enumerate(values, start=1):
      cell = self.worksheet.cell(row_index, col_index, value)
      cell.font = font
      cell.alignment = alignment
      cell.fill = fill
      cell.border = THIN_BORDER

      header_value = self.worksheet.cell(1, col_index).value
      if header_value == WER_COL_NAME:
        cell.number_format = "0.0000"
      elif header_value == HALLUCINATION_PERCENT_COL_NAME:
        cell.number_format = "0.0000"
      elif header_value == TOXIC_PERCENT_COL_NAME:
        cell.number_format = "0.0000"
      elif header_value == DURATION_COL_NAME:
        cell.number_format = "0.000"
      elif header_value == TRANSCRIPT_DENSITY_COL_NAME:
        cell.number_format = "0.00"

  # === end modification for S, I, D columns

  def apply_styling(self) -> None:
    from styling import style_metrics_columns
    style_metrics_columns(self.worksheet)

class SummaryWorksheetWriter:
  def __init__(self, workbook: Workbook, worksheet_name: str):
    self.workbook = workbook
    self.worksheet_name = worksheet_name
    self.worksheet: Optional[Worksheet] = None

  def create_worksheet(self, has_speaker_column: bool = False) -> Worksheet:
    if self.worksheet_name in self.workbook.sheetnames:
      del self.workbook[self.worksheet_name]
    self.worksheet = self.workbook.create_sheet(self.worksheet_name)
    self._write_header(has_speaker_column)
    return self.worksheet

  # === start modification for reqs column - worksheet_writer.py (SummaryWorksheetWriter)
  def _write_header(self, has_speaker_column: bool) -> None:
    headers = [METADATA_COL_NAME]
    if has_speaker_column:
      headers.append("Speaker")
    headers.extend([
      AVERAGE_WER_COL_NAME,
      SESSION_COUNT_COL_NAME,
      AVG_HALLUCINATION_PERCENT_COL_NAME,
      #REQS_COL_NAME,
      AVERAGE_TRX_DENSITY_COL_NAME,
      AVG_TOXIC_PERCENT_COL_NAME
    ])

    for col, title in enumerate(headers, start=1):
      cell = self.worksheet.cell(1, col, title)
      cell.font = Font(name="Aptos", size=9, bold=True)
      cell.alignment = Alignment("left", "center")
  # === end modification for reqs column

  # === start modification for Both row - worksheet_writer.py (SummaryWorksheetWriter.write_summary_row)
  def write_summary_row(self, summary: SummaryData, row_num: int, has_speaker_column: bool) -> None:
    values = [summary.metadata]
    if has_speaker_column:
      values.append(summary.speaker)
    values.extend([
      summary.average_wer,
      summary.session_count,
      summary.average_hallucination_percent,
      #summary.reqs,
      summary.average_transcript_density,
      summary.toxic_percent
    ])

    # Check if this is the "Both" row for special styling
    is_both_row = has_speaker_column and summary.speaker == BOTH_VALUE

    for col, value in enumerate(values, start=1):
      cell = self.worksheet.cell(row_num, col, value)
      cell.font = Font(name="Aptos", size=9)
      cell.border = THIN_BORDER

      if is_both_row:
        # Special styling for "Both" row
        cell.alignment = Alignment("left", "center")
        cell.fill = PatternFill("solid", fgColor="A5E06C")
        # "num sessions" column (col 4 with speaker column) should be integer format
        session_col = 4 if has_speaker_column else 3
        if col == session_col:
          cell.number_format = "0"
        else:
          cell.number_format = "0.000"
      else:
        # Standard styling for Customer/Agent rows
        cell.alignment = Alignment("left", "center")
        cell.fill = PatternFill(
          "solid",
          fgColor=CALL_LEVEL_BKGND_COLOR if CALL_LEVEL_GRANULARITY else SAMPLE_LEVEL_BKGND_COLOR
        )

        wer_col = 3 if has_speaker_column else 2
        halluc_col = 5 if has_speaker_column else 4
        density_col = 6 if has_speaker_column else 5
        toxic_col = 7 if has_speaker_column else 6

        if col == wer_col or col == halluc_col:
          cell.number_format = "0.0000"
        elif col == density_col:
          cell.number_format = "0.00"
        elif col == toxic_col:
          cell.number_format = "0.0000"
  # === end modification for Both row

  # === start modification for reqs column - worksheet_writer.py (write_overall_toxic_percent)
  def write_overall_toxic_percent(self, metrics_worksheet: Worksheet, has_speaker_column: bool) -> None:
    """
    Write overall toxic percentage to cell H4 (or G4 if no speaker column).
    Calculation: 100 * (total toxic tokens) / (total model tokens) for Customer + Agent
    """
    from openpyxl.styles import PatternFill
    from constants import (
      SECTION_COL_NAME,
      MODEL_TOKS_COL_NAME,
      TOXIC_COL_NAME,
      CUSTOMER_VALUE,
      AGENT_VALUE
    )

    # Find column indices in metrics worksheet
    header_map = {c.value: idx for idx, c in enumerate(metrics_worksheet[1], start=1)}
    section_index = header_map.get(SECTION_COL_NAME, 0) - 1
    model_toks_index = header_map.get(MODEL_TOKS_COL_NAME, 0) - 1
    toxic_index = header_map.get(TOXIC_COL_NAME, 0) - 1

    if section_index < 0 or model_toks_index < 0 or toxic_index < 0:
      return  # Required columns not found

    total_model_tokens = 0
    total_toxic_count = 0

    # Iterate through metrics worksheet rows
    for row in metrics_worksheet.iter_rows(min_row=2, values_only=True):
      if len(row) <= max(section_index, model_toks_index, toxic_index):
        continue

      section_value = row[section_index]

      # Only process "Customer" and "Agent" rows (exclude "Customer1", "Agent2", etc.)
      if section_value in (CUSTOMER_VALUE, AGENT_VALUE):
        model_toks = row[model_toks_index]
        toxic_text = row[toxic_index]

        # Add model tokens
        if isinstance(model_toks, (int, float)):
          total_model_tokens += model_toks

        # Count toxic tokens from comma-separated list
        if toxic_text and isinstance(toxic_text, str):
          toxic_tokens = [t.strip() for t in toxic_text.split(',') if t.strip()]
          total_toxic_count += len(toxic_tokens)

    # Calculate overall toxic percentage
    overall_toxic_percent = (100 * total_toxic_count / total_model_tokens) if total_model_tokens > 0 else 0.0

    # Determine the column for toxic% (H if has_speaker, G if not) - shifted due to Reqs column
    toxic_col_letter = 'H' if has_speaker_column else 'G'
    cell_address = f'{toxic_col_letter}4'
  # === end modification for reqs column
===FILE_END===:worksheet_writer.py