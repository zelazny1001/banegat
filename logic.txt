# create_confusion_matrix.py

import pandas as pd
from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment, Font, Border, Side
from typing import Any, Dict, Tuple, List


# globals
SPREADSHEET_PATH: str = "accuracy-test.xlsx"
EXCLUDED_INTENT: str = "OTHER"
GROUND_TRUTH_WORKSHEET: str = "GroundTruth"
MODELS: List[str] = ["v1", "v2", "transformer"]
RESULT_WORKSHEET: str = "results"
PIVOT_WORKSHEET: str = "pivot"
DETAILS_WORKSHEET: str = "details"

def get_intent_column(df: pd.DataFrame) -> pd.Series:
    for col in df.columns:
        if col.strip().lower() == "intent":
            return df[col]
    raise ValueError("No 'Intent' column found in the DataFrame.")

def compute_confusion_by_class(
    truth: pd.Series,
    pred: pd.Series
) -> Dict[Any, Dict[str, int]]:
    """
    Parameters:
        truth: pd.Series of true labels
        pred:  pd.Series of predicted labels
    Returns:
        A dictionary mapping each label to a confusion dictionary:
            {
                label1: {"TP": int, "FP": int, "FN": int, "TN": int},
                label2: {"TP": int, "FP": int, "FN": int, "TN": int},
                ...
            }
    """
    labels = sorted(set(truth) | set(pred))
    confusion: Dict[Any, Dict[str, int]] = {
        label: {"TP": 0, "FP": 0, "FN": 0, "TN": 0} for label in labels
    }
    for t, p in zip(truth, pred):
        for label in labels:
            if t == label and p == label:
                confusion[label]["TP"] += 1
            elif t != label and p == label:
                confusion[label]["FP"] += 1
            elif t == label and p != label:
                confusion[label]["FN"] += 1
            elif t != label and p != label:
                confusion[label]["TN"] += 1
    return confusion


def compute_metrics_from_confusion(
    confusion: Dict[str, int]
) -> Tuple[float, float, float, float, float]:
    TP: int = confusion["TP"]
    FP: int = confusion["FP"]
    FN: int = confusion["FN"]
    TN: int = confusion["TN"]
    total: int = TP + FP + FN + TN

    precision: float = TP / (TP + FP) if (TP + FP) > 0 else 0.0
    recall: float = TP / (TP + FN) if (TP + FN) > 0 else 0.0
    f1: float = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    f05: float = (1.25 * precision * recall) / (0.25 * precision + recall) if (precision + recall) > 0 else 0.0
    accuracy: float = (TP + TN) / total if total > 0 else 0.0

    return precision, recall, f1, f05, accuracy


def print_confusion_summary(
    model: str,
    confusion: Dict[Any, Dict[str, int]]
) -> None:
    print(f"\nModel: {model}")
    for label, counts in confusion.items():
        print(
            f"  Class '{label}': "
            f"TP={counts['TP']}, FP={counts['FP']}, FN={counts['FN']}, TN={counts['TN']}"
        )


def compute_micro_macro_metrics(
    confusion_dict: Dict[Any, Dict[str, int]]
) -> Tuple[
    Tuple[float, float, float, float, float],
    Tuple[float, float, float, float, float]
]:
    """
    Parameters:
        confusion_dict: dict mapping each label to its confusion-dict
    Returns:
        micro:   (micro_precision, micro_recall, micro_f1, micro_f0.5, micro_accuracy)
        macro:   (macro_precision, macro_recall, macro_f1, macro_f0.5, macro_accuracy)
    Note:
        Excludes the EXCLUDED_INTENT label from both micro and macro calculations.
    """
    total_conf: Dict[str, int] = {"TP": 0, "FP": 0, "FN": 0, "TN": 0}
    precisions: List[float] = []
    recalls: List[float] = []
    f1s: List[float] = []
    f05s: List[float] = []
    accs: List[float] = []

    for label, conf in confusion_dict.items():
        if label == EXCLUDED_INTENT:
            continue

        total_conf["TP"] += conf["TP"]
        total_conf["FP"] += conf["FP"]
        total_conf["FN"] += conf["FN"]
        total_conf["TN"] += conf["TN"]

        p, r, f1, f05, acc = compute_metrics_from_confusion(conf)
        precisions.append(p)
        recalls.append(r)
        f1s.append(f1)
        f05s.append(f05)
        accs.append(acc)

    micro: Tuple[float, float, float, float, float] = compute_metrics_from_confusion(total_conf)
    macro: Tuple[float, float, float, float, float] = (
        sum(precisions) / len(precisions) if precisions else 0.0,
        sum(recalls) / len(recalls) if recalls else 0.0,
        sum(f1s) / len(f1s) if f1s else 0.0,
        sum(f05s) / len(f05s) if f05s else 0.0,
        sum(accs) / len(accs) if accs else 0.0
    )

    return micro, macro


def create_pivot(
    results_df: pd.DataFrame,
    filepath: str
) -> None:
    pivot_df: pd.DataFrame = results_df.set_index(["Model", "Intent"]).transpose()
    pivot_df = pivot_df[~pivot_df.index.isin({"TP", "FP", "FN", "TN"})]
    pivot_df = pivot_df.astype(float).round(4)

    with pd.ExcelWriter(filepath, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
        pivot_df.to_excel(writer, sheet_name=PIVOT_WORKSHEET)
        ws = writer.book[PIVOT_WORKSHEET]

        aptos: Font = Font(name="Aptos", size=9)
        bold:   Font = Font(name="Aptos", size=9, bold=True)
        light_gray_border: Border = Border(
            left=Side(style="thin", color="D3D3D3"),
            right=Side(style="thin", color="D3D3D3"),
            top=Side(style="thin", color="D3D3D3"),
            bottom=Side(style="thin", color="D3D3D3")
        )
        ws["A1"].value = None
        ws.column_dimensions["A"].width = 13
        for col in range(2, ws.max_column + 1):
            ws.column_dimensions[get_column_letter(col)].width = 18
        for row_idx, row in enumerate(ws.iter_rows()):
            for col_idx, cell in enumerate(row):
                if col_idx > 0:
                    cell.number_format = '0.00%'
                cell.font = aptos if col_idx == 0 else bold if row_idx == 0 else aptos
                cell.alignment = Alignment(
                    horizontal="right" if col_idx == 0 else "center",
                    vertical="center"
                )
                cell.border = light_gray_border


def doSummaryCalculations(
    excel_spreadsheet_filepath: str
) -> str:
    """
    Parameters:
        excel_spreadsheet_filepath: str path to the Excel file containing ground truth and model sheets
    Returns:
        str: the same filepath, after writing the 'results' and 'pivot' sheets
    Note:
        For each label != EXCLUDED_INTENT, compute binary metrics. EXCLUDED_INTENT is still present in confusion,
        but not used for micro/macro or as a binary target.
    """
    truth_df: pd.DataFrame = pd.read_excel(
        excel_spreadsheet_filepath,
        sheet_name=GROUND_TRUTH_WORKSHEET,
        engine="openpyxl"
    )
    truth_series: pd.Series = get_intent_column(truth_df)
    results_list: List[Dict[str, Any]] = []

    for model in MODELS:
        model_df: pd.DataFrame = pd.read_excel(
            excel_spreadsheet_filepath,
            sheet_name=model,
            engine="openpyxl"
        )
        model_series: pd.Series = get_intent_column(model_df)

        if len(model_series) != len(truth_series):
            raise ValueError(f"Row count mismatch between {GROUND_TRUTH_WORKSHEET} and {model}.")

        confusion: Dict[Any, Dict[str, int]] = compute_confusion_by_class(truth_series, model_series)
        print_confusion_summary(model, confusion)  # diagnostic output

        # Compute micro/macro once per model, excluding EXCLUDED_INTENT
        micro, macro = compute_micro_macro_metrics(confusion)

        # For each label except EXCLUDED_INTENT, compute binary metrics
        for label, binary_conf in confusion.items():
            if label == EXCLUDED_INTENT:
                continue

            TP: int = binary_conf["TP"]
            FP: int = binary_conf["FP"]
            FN: int = binary_conf["FN"]
            TN: int = binary_conf["TN"]

            precision, recall, f1, f05, accuracy = compute_metrics_from_confusion(binary_conf)

            results_list.append({
                "Model": model,
                "Intent": label,
                "TP": TP,
                "FP": FP,
                "FN": FN,
                "TN": TN,
                "Binary Accuracy": accuracy,
                "Binary Precision": precision,
                "Binary Recall": recall,
                "Binary F1": f1,
                "Binary F0.5": f05,
                "Micro Precision": micro[0],
                "Micro Recall": micro[1],
                "Micro F1": micro[2],
                "Micro F0.5": micro[3],
                "Micro Accuracy": micro[4],
                "Macro Precision": macro[0],
                "Macro Recall": macro[1],
                "Macro F1": macro[2],
                "Macro F0.5": macro[3],
                "Macro Accuracy": macro[4]
            })

    results_df: pd.DataFrame = pd.DataFrame(results_list)
    col_widths: Dict[str, int] = {
        "TP": 7, "FP": 7, "FN": 7, "TN": 7,
        "Binary Accuracy": 12, "Binary Precision": 12,
        "Binary Recall": 12, "Binary F1": 12, "Binary F0.5": 12
    }
    for col in results_df.columns:
        if col.startswith("Micro") or col.startswith("Macro"):
            col_widths[col] = 14
        elif col not in col_widths:
            col_widths[col] = 20

    with pd.ExcelWriter(excel_spreadsheet_filepath, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
        results_df.to_excel(writer, sheet_name=RESULT_WORKSHEET, index=False)
        ws = writer.book[RESULT_WORKSHEET]

        for i, col in enumerate(results_df.columns, 1):
            ws.column_dimensions[get_column_letter(i)].width = col_widths.get(col, 20)

        for row_idx, row in enumerate(ws.iter_rows()):
            for col_idx, cell in enumerate(row):
                col_name: str = results_df.columns[col_idx]
                if row_idx > 0 and col_name not in {"TP", "FP", "FN", "TN", "Model", "Intent"}:
                    cell.number_format = '0.00%'
                cell.font = Font(name="Aptos", size=9)
                cell.alignment = Alignment(horizontal="center", vertical="center")

    create_pivot(results_df, excel_spreadsheet_filepath)
    return excel_spreadsheet_filepath


def write_details_worksheet(filepath: str) -> None:
    truth_df: pd.DataFrame = pd.read_excel(
        filepath, sheet_name=GROUND_TRUTH_WORKSHEET, engine="openpyxl"
    )
    truth_series: pd.Series = get_intent_column(truth_df)
    rows: List[Dict[str, Any]] = []

    for model in MODELS:
        model_df: pd.DataFrame = pd.read_excel(filepath, sheet_name=model, engine="openpyxl")
        model_series: pd.Series = get_intent_column(model_df)
        confusion: Dict[Any, Dict[str, int]] = compute_confusion_by_class(truth_series, model_series)

        for label, conf in confusion.items():
            p, r, f1, f05, acc = compute_metrics_from_confusion(conf)
            rows.append({
                "Model": model,
                "Class": label,
                "TP": conf["TP"],
                "FP": conf["FP"],
                "FN": conf["FN"],
                "Precision": p,
                "Recall": r,
                "F1": f1,
                "F0.5": f05,
                "Accuracy": acc
            })

        micro, macro = compute_micro_macro_metrics(confusion)
        rows.append({
            "Model": model,
            "Class": "MICRO",
            "TP": sum(conf["TP"] for label, conf in confusion.items() if label != EXCLUDED_INTENT),
            "FP": sum(conf["FP"] for label, conf in confusion.items() if label != EXCLUDED_INTENT),
            "FN": sum(conf["FN"] for label, conf in confusion.items() if label != EXCLUDED_INTENT),
            "Precision": micro[0],
            "Recall": micro[1],
            "F1": micro[2],
            "F0.5": micro[3],
            "Accuracy": micro[4]
        })
        rows.append({
            "Model": model,
            "Class": "MACRO",
            "TP": "-",
            "FP": "-",
            "FN": "-",
            "Precision": macro[0],
            "Recall": macro[1],
            "F1": macro[2],
            "F0.5": macro[3],
            "Accuracy": macro[4]
        })

    df: pd.DataFrame = pd.DataFrame(rows)
    with pd.ExcelWriter(filepath, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
        df.to_excel(writer, sheet_name=DETAILS_WORKSHEET, index=False)
        ws = writer.book[DETAILS_WORKSHEET]
        ws.column_dimensions["A"].width = 18
        ws.column_dimensions["B"].width = 22
        for col_letter in "CDEFGHIJ":
            ws.column_dimensions[col_letter].width = 10

        for row_idx, row in enumerate(ws.iter_rows()):
            for col_idx, cell in enumerate(row):
                if row_idx == 0:
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                elif col_idx == 0:
                    cell.alignment = Alignment(horizontal="left", vertical="center")
                elif col_idx == 1:
                    cell.alignment = Alignment(horizontal="right", vertical="center")
                else:
                    cell.alignment = Alignment(horizontal="center", vertical="center")

                if isinstance(cell.value, float):
                    cell.number_format = '0.00%'
                cell.font = Font(name="Aptos", size=9)


def main() -> None:
    updated_file: str = doSummaryCalculations(SPREADSHEET_PATH)
    write_details_worksheet(updated_file)
    print(f"Spreadsheet updated: {updated_file}")

if __name__ == "__main__":
    main()