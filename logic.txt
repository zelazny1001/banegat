# confusion_matrix.py

import pandas as pd
from collections import defaultdict
from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment

# Globals
SPREADSHEET_PATH = "accuracy-test.xlsx"
INTENT_OF_INTEREST = "INTENT_1"
GROUND_TRUTH_WORKSHEET = "GroundTruth"
MODELS = ["v1", "v2", "transformer"]
RESULT_WORKSHEET = "results"

def get_intent_column(df):
    """
    Retrieve the 'Intent' column from the DataFrame regardless of case.
    """
    for col in df.columns:
        if col.strip().lower() == "intent":
            return df[col]
    raise ValueError("No 'Intent' column found in the DataFrame.")

def compute_confusion_by_class(truth, pred):
    labels = sorted(set(truth) | set(pred))
    confusion = {label: {"TP": 0, "FP": 0, "FN": 0, "TN": 0} for label in labels}
    for t, p in zip(truth, pred):
        for label in labels:
            if t == label and p == label:
                confusion[label]["TP"] += 1
            elif t != label and p == label:
                confusion[label]["FP"] += 1
            elif t == label and p != label:
                confusion[label]["FN"] += 1
            elif t != label and p != label:
                confusion[label]["TN"] += 1
    return confusion

def compute_metrics_from_confusion(conf):
    TP = conf["TP"]
    FP = conf["FP"]
    FN = conf["FN"]
    TN = conf["TN"]
    total = TP + FP + FN + TN
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    f05 = (1.25 * precision * recall) / (0.25 * precision + recall) if (precision + recall) > 0 else 0
    accuracy = (TP + TN) / total if total > 0 else 0
    return precision, recall, f1, f05, accuracy

def compute_micro_macro_metrics(confusion_dict):
    totals = defaultdict(int)
    precisions, recalls, f1s, f05s, accs = [], [], [], [], []

    for label, conf in confusion_dict.items():
        for k in conf:
            totals[k] += conf[k]
        p, r, f1, f05, acc = compute_metrics_from_confusion(conf)
        precisions.append(p)
        recalls.append(r)
        f1s.append(f1)
        f05s.append(f05)
        accs.append(acc)

    micro = compute_metrics_from_confusion(totals)
    macro = (
        sum(precisions)/len(precisions),
        sum(recalls)/len(recalls),
        sum(f1s)/len(f1s),
        sum(f05s)/len(f05s),
        sum(accs)/len(accs)
    )
    return micro, macro

def doSummaryCalculations(excel_spreadsheet_filepath, intent_of_interest):

    # get ground truth data and extract the Intent column.
    truth_df = pd.read_excel(excel_spreadsheet_filepath, sheet_name=GROUND_TRUTH_WORKSHEET, engine="openpyxl")
    truth_series = get_intent_column(truth_df)

    results_list = []

    for model in MODELS:
        model_df = pd.read_excel(excel_spreadsheet_filepath, sheet_name=model, engine="openpyxl")
        model_series = get_intent_column(model_df)

        # basic sanity check that the number of rows matches between ground truth and predictions.
        if len(model_series) != len(truth_series):
            raise ValueError(f"Row count mismatch between {GROUND_TRUTH_WORKSHEET} and {model}.")

        # get counts based on the intent of interest.
        TP = ((truth_series == intent_of_interest) & (model_series == intent_of_interest)).sum()
        FP = ((truth_series != intent_of_interest) & (model_series == intent_of_interest)).sum()
        FN = ((truth_series == intent_of_interest) & (model_series != intent_of_interest)).sum()
        TN = ((truth_series != intent_of_interest) & (model_series != intent_of_interest)).sum()

        accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0
        fnr = 1 - recall

        confusion = compute_confusion_by_class(truth_series, model_series)
        micro, macro = compute_micro_macro_metrics(confusion)

        results_list.append({
            "Model": model,
            "True Positives": TP,
            "False Positives": FP,
            "False Negatives": FN,
            "True Negatives": TN,
            "Accuracy": f"{accuracy * 100:.2f}%",
            "Precision": f"{precision * 100:.2f}%",
            "Recall": f"{recall * 100:.2f}%",
            "False Positive Rate": f"{fpr * 100:.2f}%",
            "False Negative Rate": f"{fnr * 100:.2f}%",
            "Micro Precision": f"{micro[0] * 100:.2f}%",
            "Micro Recall": f"{micro[1] * 100:.2f}%",
            "Micro F1": f"{micro[2] * 100:.2f}%",
            "Micro F0.5": f"{micro[3] * 100:.2f}%",
            "Micro Accuracy": f"{micro[4] * 100:.2f}%",
            "Macro Precision": f"{macro[0] * 100:.2f}%",
            "Macro Recall": f"{macro[1] * 100:.2f}%",
            "Macro F1": f"{macro[2] * 100:.2f}%",
            "Macro F0.5": f"{macro[3] * 100:.2f}%",
            "Macro Accuracy": f"{macro[4] * 100:.2f}%"
        })

    results_df = pd.DataFrame(results_list)
    with pd.ExcelWriter(excel_spreadsheet_filepath, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
        results_df.to_excel(writer, sheet_name=RESULT_WORKSHEET, index=False)
        worksheet = writer.book[RESULT_WORKSHEET]

        for col in range(1, worksheet.max_column + 1):
            col_letter = get_column_letter(col)
            worksheet.column_dimensions[col_letter].width = 20

        for row in worksheet.iter_rows():
            for cell in row:
                cell.alignment = Alignment(horizontal="center", vertical="center")

    return excel_spreadsheet_filepath


def main():
    updated_file = doSummaryCalculations(SPREADSHEET_PATH, INTENT_OF_INTEREST)
    print(f"Spreadsheet updated: {updated_file}")


if __name__ == "__main__":
    main()